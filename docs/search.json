[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog post #2: logistic regression.\n\n\n\n\n\n\nMar 29, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #1: perceptrons.\n\n\n\n\n\n\nFeb 20, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code is here.\nIn this blog post I have implemented logistic regression.\n\n# from solutions.logistic import LogisticRegression # your source code\nfrom logreg import LogisticRegression\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ndef draw_line(w, x_min, x_max, color=\"black\", ax=None, alpha=1):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  if ax is None:\n    plt.plot(x, y, color = color, alpha=alpha)\n  else:\n    ax.plot(x, y, color = color, alpha=alpha)"
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiments",
    "href": "posts/logistic-regression/index.html#experiments",
    "title": "Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\n\nExperiment 1: Alpha Too Large\nIn this experiment I train two models on the same data with the same intitial weights and the same number of epochs in order to demonstrate what can happen when alpha is too high.\nIn the charts and graohs below, the blue corresponds to the model trained with alpha=10 and the red corresponds with alpha=20.\nIn the scatter plots, I plot all of the best fit lines, increasing the opacity of the lines to 100% as we approach the final epoch. In the alpha=10 plot, we see that the line quickly finds roughly where it needs to be and then makes very small changes it approaches the optimal line. In the alpha=20 plot, the best fit lines are all over the place. We see why if we look at the loss graph. Everytime the alpha=20 model gets close to a very low loss, it passes the minimum and the loss jumps back up again.\n\nnp.random.seed(38532555)\np_features = 3\nX, y = make_blobs(n_samples = 80, n_features = p_features - 1, centers = [(-1.5, -1.5), (-2, 1.5)])\n\nsteps=300\n# train normal logistic regression\nreg_alpha_LR = LogisticRegression()\nreg_alpha_LR.fit(X, y, initial_w=np.array([1, 1, -1]), alpha = 10, max_epochs = steps, track_w=True)\n\n# train high alpha logistic regression\nhigh_alpha_LR = LogisticRegression()\nhigh_alpha_LR.fit(X, y, initial_w=np.array([1, 1, -1]), alpha = 20, max_epochs = steps, track_w=True)\n\nfig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 12))\n\n# figs 1 and 2\nfig1 = ax1.scatter(X[:,0], X[:,1], c = y)\nfig4 = ax4.scatter(X[:,0], X[:,1], c = y)\nfor i, (wi1, wi2) in enumerate(zip(reg_alpha_LR.w_history, high_alpha_LR.w_history)):\n    fig1 = draw_line(wi1, -3.5, 0, \"blue\", ax=ax1, alpha=((i+1+5)/(steps+5)))\n    fig4 = draw_line(wi2, -3.5, 0, \"red\", ax=ax4, alpha=((i+1+5)/(steps+5)))\n    \nax1.set(xlabel='Feauture 1', ylabel='Feauture 2', title=\"alpha=10\")\nax4.set(xlabel='Feauture 1', ylabel='Feauture 2', title=\"alpha=20\")\n\n# fig 2\nnum_steps = len(reg_alpha_LR.loss_history)\nax2.plot(np.arange(num_steps) + 1, reg_alpha_LR.loss_history, color=\"blue\")\nax2.set(xlabel='Epoch', ylabel='Loss')\n\n#fig 5\nnum_steps = len(high_alpha_LR.loss_history)\nax5.plot(np.arange(num_steps) + 1, high_alpha_LR.loss_history, color=\"red\")\nax5.set(xlabel='Epoch', ylabel='Loss')\n\n# fig 3\nnum_steps = len(reg_alpha_LR.score_history)\nax3.plot(np.arange(num_steps) + 1, reg_alpha_LR.score_history, color=\"blue\")\nax3.set(xlabel='Epoch', ylabel='Score')\n\n# fig 6\nnum_steps = len(high_alpha_LR.score_history)\nax6.plot(np.arange(num_steps) + 1, high_alpha_LR.score_history, color=\"red\")\nax6.set(xlabel='Epoch', ylabel='Score')\nNone\n\n\n\n\n\n\nExperiment 2 and 3: Batch Size and Momentum\nIn this experiment I will demonstrate how small batch gradient descent can boost the speed of an algorithm, and then how momentum can boost speed on top of that.\nAll models are trained with the same data, starting weights, and number of epochs.\nThe data has five features, so I will not be showing any best fit lines. Instead, I chart the loss and score histories. These cleary show how the non sped up gradient descent has not even come close to reaching a minimum after 250 epochs. The stochasitc descent took over 200 epochs, but the stochastic with momentum took only about 50.\n\nnp.random.seed(385325)\np_features = 6\nX, y = make_blobs(n_samples = 400, n_features = p_features - 1, centers = [(-1, 1, 0, -1, -1), (1, -1, 2, 1, 1)])\n\nsteps=250\n# train normal logistic regression\nreg_LR = LogisticRegression()\nreg_LR.fit(X, y, initial_w=[-1, 4, -4, 1, 3, -1], alpha = .01, max_epochs = steps)\n\n# train small batch logistic regression\nstochastic_LR = LogisticRegression()\nstochastic_LR.fit_stochastic(X, y, initial_w=[-1, 4, -4, 1, 3, -1], batch_size=25, alpha = .01, max_epochs = steps)\n\n# train small batch logistic regression with momentum\nstoch_momentum_LR = LogisticRegression()\nstoch_momentum_LR.fit_stochastic(X, y, initial_w=[-1, 4, -4, 1, 3, -1], momentum=.8, batch_size=25, alpha = .01, max_epochs = steps)\n\nfig, (ax2, ax3) = plt.subplots(1, 2, figsize=(18, 6))\n\n# fig 2\nnum_steps = len(reg_LR.loss_history)\nax2.plot(np.arange(num_steps) + 1, reg_LR.loss_history, label=\"regular gradient\")\nax2.plot(np.arange(num_steps) + 1, stochastic_LR.loss_history, label=\"stochastic\")\nax2.plot(np.arange(num_steps) + 1, stoch_momentum_LR.loss_history, label=\"stochastic w/momentum\")\nax2.set(xlabel='Epoch', ylabel='Loss')\nlegend = ax2.legend() \n\n# fig 3\nnum_steps = len(reg_LR.score_history)\nax3.plot(np.arange(num_steps) + 1, reg_LR.score_history, label=\"regular gradient\")\nax3.plot(np.arange(num_steps) + 1, stochastic_LR.score_history, label=\"stochastic\")\nax3.plot(np.arange(num_steps) + 1, stoch_momentum_LR.score_history, label=\"stochastic w/momentum\")\nax3.set(xlabel='Epoch', ylabel='Score')\nlegend = ax3.legend()"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Experiment 1: Linearly Separable 2D Data\nThis experiment looks at linearly seperable data with 2 features. The perceptron is able to quickly find a line that achieves 100% accuracy.\n\nnp.random.seed(12345)\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.8, -1.9), (1.6, 1.7)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 2: NOT Linearly Separable 2D Data\nThis experiment looks at data with 2 features that is not linearly seperable. The perceptron is not able to find a set of weights (a line) that can accuractely predict all of the data points because the data because this data is not linearly seperable.\nAlso, one of the qualities of the perceptron is that it does not converge, and we can see in the graph below containing the accuracies over the iterations that the accuracy continues to jump up and down until it reaches the max number of steps.\n\nnp.random.seed(54321)\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 3: Linearly Separable (?) 5D Data\nThis data containing 5 features turns out to be linearly seperable. We can tell it is linearly seperable without visualizing it because the perceptron is able to find line weights that make predictions with an accuracy of 100%.\n\nnp.random.seed(12321)\nn = 100\np_features = 6\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.75, -1.75), (1.75, 1.75)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThis data is linearly seperable. The perceptron was able to achieve 100% accuracy!\n\n\nRuntime Complexity of the Update Step\nThe update contains multiple individual steps occuring in sequence. We will look at them all individually\nFirst lets look at making the prediction: np.sign(np.dot(self.w, X_[i])). This operation does a 1xp dot px1 operation. This specific step is O(p).\nNext, lets look at the forming of the gate: (y_[i] * y_pred) < 0. This is clearly O(1).\nNow for the update: self.w + false_check * (y_[i] * X_[i]). The major component here is the scalar * vector operation. Since the vector is of length p, this is O(p) as well.\nWe could end there and be done, but we check the overall accuracy as part of the update step: (np.dot(self.w, X_.T) > 0). This is a dot product operation of 1xp vector and a pxn matrix. Therefore, this step has a runtime of O(pn).\nSince all of the steps are done in sequence, the runtime for the total update step is O(pn)."
  }
]