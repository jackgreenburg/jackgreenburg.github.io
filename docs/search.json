[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog post #1: perceptrons.\n\n\n\n\n\n\nFeb 20, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Experiment 1: Linearly Separable 2D Data\nThis experiment looks at linearly seperable data with 2 features. The perceptron is able to quickly find a line that achieves 100% accuracy.\n\nnp.random.seed(12345)\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.8, -1.9), (1.6, 1.7)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 2: NOT Linearly Separable 2D Data\nThis experiment looks at data with 2 features that is not linearly seperable. The perceptron is not able to find a set of weights (a line) that can accuractely predict all of the data points because the data because this data is not linearly seperable.\nAlso, one of the qualities of the perceptron is that it does not converge, and we can see in the graph below containing the accuracies over the iterations that the accuracy continues to jump up and down until it reaches the max number of steps.\n\nnp.random.seed(54321)\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 3: Linearly Separable (?) 5D Data\nThis data containing 5 features turns out to be linearly seperable. We can tell it is linearly seperable without visualizing it because the perceptron is able to find line weights that make predictions with an accuracy of 100%.\n\nnp.random.seed(12321)\nn = 100\np_features = 6\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.75, -1.75), (1.75, 1.75)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThis data is linearly seperable. The perceptron was able to achieve 100% accuracy!\n\n\nRuntime Complexity of the Update Step\nThe update contains multiple individual steps occuring in sequence. We will look at them all individually\nFirst lets look at making the prediction: np.sign(np.dot(self.w, X_[i])). This operation does a 1xp dot px1 operation. This specific step is O(p).\nNext, lets look at the forming of the gate: (y_[i] * y_pred) < 0. This is clearly O(1).\nNow for the update: self.w + false_check * (y_[i] * X_[i]). The major component here is the scalar * vector operation. Since the vector is of length p, this is O(p) as well.\nWe could end there and be done, but we check the overall accuracy as part of the update step: (np.dot(self.w, X_.T) > 0). This is a dot product operation of 1xp vector and a pxn matrix. Therefore, this step has a runtime of O(pn).\nSince all of the steps are done in sequence, the runtime for the total update step is O(pn)."
  }
]