[
  {
    "objectID": "posts/kernel-logistic-regression/index.html",
    "href": "posts/kernel-logistic-regression/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Source code is here.\nIn this blog post I have implemented kernel logistic regression. Kernel logistic regression is a lot like regular linear logistic regression except we modify the data beforehand in order to still be able to use linear regression on nonlinear data.\nFor those that are curious, my loss function remains largely unchanged. I simply use the kernel matrix instead of the regular nxp data matrix. I use all numpy functions that performs the operations on all of the rows.\n\nfrom kernel_logreg import KernelLogisticRegression\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\nfrom scipy.optimize import minimize\nfrom sklearn.datasets import make_blobs, make_circles, make_moons\nfrom sklearn.metrics.pairwise import rbf_kernel\n\n\nnp.seterr(all='ignore')\n\ndef draw_line(w, x_min, x_max, color=\"black\", ax=None, alpha=1):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    if ax is None:\n        plt.plot(x, y, color = color, alpha=alpha)\n    else:\n        ax.plot(x, y, color = color, alpha=alpha)\n\ndef construct_plot(X, y, clf, ax=None, title=\"Accuracy = \"):\n    plot_decision_regions(X, y, clf=clf, ax=ax)\n    if ax is None: ax = plt.gca()\n    title = ax.set(title = f\"{title}{(KLR.predict(X) == y).mean()}\",\n                        xlabel = \"Feature 1\", \n                        ylabel = \"Feature 2\")"
  },
  {
    "objectID": "posts/kernel-logistic-regression/index.html#experiments",
    "href": "posts/kernel-logistic-regression/index.html#experiments",
    "title": "Kernel Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\n\nExperiment 1: Basic Check\nHere I am just confirming that all of my code actually works.\nRunning a test on moon shaped data with gamma=0.1 I get a model that is a pretty good fit for both train and test data.\n\nX_train, y_train = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X_train, y_train)\n\nX_test, y_test = make_moons(100, shuffle = True, noise = 0.2)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\nconstruct_plot(X_train, y_train, KLR, ax1, \"Training Accuracy = \")\nconstruct_plot(X_test, y_test, KLR, ax2, \"Test Accuracy = \")\n\n\n\n\n\n\nExperiment 2: Choosing Gamma\nIn this experiment I will demonstrate the importance of choosing a good gamma value. A gamma too low will underfit the data, but a gamma too high will overfit the data. As we can see values for gamma above 1 begin to lose accuracy on the test set.\n\nX_train, y_train = make_moons(80, shuffle = True, noise = 0.2)\nX_test, y_test = make_moons(80, shuffle = True, noise = 0.2)\ngammas = [.01, .1, 1, 10, 100, 1000, 10000]\nfig, axarr = plt.subplots(len(gammas), 2, figsize=(12, 6*len(gammas)))\n\nfor i, gamma in enumerate(gammas):\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"gamma={gamma} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"gamma={gamma} Test Accuracy = \")\n\n\n\n\n\n\nExperiment 3: Varied Noise\nIn this experiment I vary the noise paramater of the make_moons function, and I test what values of gamma are best with increased noise.\nIn the last experiment we tested with noise=.2 and saw that a gamma value of around 1 was sufficient.\n\nTesting with Noise=.05\nFrom this test we can gather that when there is very little noise (and very little difference between the train and test data), we just need a high enough gamma to follow the shape.\n\nX_train, y_train = make_moons(80, shuffle = True, noise = 0.05)\nX_test, y_test = make_moons(80, shuffle = True, noise = 0.05)\ngammas = [.01, .1, 1, 10]\nfig, axarr = plt.subplots(len(gammas), 2, figsize=(12, 6*len(gammas)))\n\nfor i, gamma in enumerate(gammas):\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"gamma={gamma} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"gamma={gamma} Test Accuracy = \")\n\n\n\n\n\n\nTesting with Noise=.04\nIn this test we see that a gamma of .1 does a the best at matching the shape we want. Higher than .1 is overfit and below .1 is underfit.\n\nX_train, y_train = make_moons(80, shuffle = True, noise = 0.4)\nX_test, y_test = make_moons(80, shuffle = True, noise = 0.4)\ngammas = [.01, .1, 1, 10]\nfig, axarr = plt.subplots(len(gammas), 2, figsize=(12, 6*len(gammas)))\n\nfor i, gamma in enumerate(gammas):\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"gamma={gamma} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"gamma={gamma} Test Accuracy = \")\n\n\n\n\n\n\nConclusions from Experiment 3\nOverall, I don’t have enough evidence to suggest that the best value for gamma depends on the noise level. This generally matches what I have seen in the documentation that assigns gamma based off of the size of the training set.\n\n\n\nExperiment 4: Other Shapes\n\nTesting with gamma=.1\n\nnoises = [0, .1, .2]\nfig, axarr = plt.subplots(len(noises), 2, figsize=(12, 6*len(noises)))\ngamma = .1\nfor i, noise in enumerate(noises):\n    X_train, y_train = make_circles(50, shuffle = True, noise = noise)\n    X_test, y_test = make_circles(50, shuffle = True, noise = noise)\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"noise={noise} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"noise={noise} Test Accuracy = \")\n\n\n\n\n\n\nTesting with gamma=1\n\nnoises = [0, .1, .2]\nfig, axarr = plt.subplots(len(noises), 2, figsize=(12, 6*len(noises)))\ngamma = 1\nfor i, noise in enumerate(noises):\n    X_train, y_train = make_circles(50, shuffle = True, noise = noise)\n    X_test, y_test = make_circles(50, shuffle = True, noise = noise)\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"noise={noise} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"noise={noise} Test Accuracy = \")\n\n\n\n\n\n\nTesting with gamma=10\nA gamma value of 10 is too high with\n\nnoises = [0, .1, .2]\nfig, axarr = plt.subplots(len(noises), 2, figsize=(12, 6*len(noises)))\ngamma = 10\nfor i, noise in enumerate(noises):\n    X_train, y_train = make_circles(50, shuffle = True, noise = noise)\n    X_test, y_test = make_circles(50, shuffle = True, noise = noise)\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"noise={noise} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"noise={noise} Test Accuracy = \")\n\n\n\n\n\n\nConclusions from Experiment 4\nWhen there is no noise pretty much any reasonable gamma will do, but once there starts to be signficant noise, the lower gamma value does better on this set."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code is here.\nIn this blog post I have implemented logistic regression. I will show what happens when alpha is set too high (it doesn’t converge), and I will also test stochastic gradient descent and gradient descent with momentum in order to show how it they converge faster but less smoothly.\n\nfrom logreg import LogisticRegression\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ndef draw_line(w, x_min, x_max, color=\"black\", ax=None, alpha=1):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  if ax is None:\n    plt.plot(x, y, color = color, alpha=alpha)\n  else:\n    ax.plot(x, y, color = color, alpha=alpha)"
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiments",
    "href": "posts/logistic-regression/index.html#experiments",
    "title": "Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\n\nExperiment 1: Alpha Too Large\nIn this experiment I train two models on the same data with the same initial weights and the same number of epochs in order to demonstrate what can happen when alpha is too high.\nIn the charts and graphs below, the blue corresponds to the model trained with alpha=10 and the red corresponds with alpha=20.\n\nnp.random.seed(38532555)\np_features = 3\nX, y = make_blobs(n_samples = 80, n_features = p_features - 1, centers = [(-1.5, -1.5), (-2, 1.5)])\n\nsteps=300\n# train normal logistic regression\nreg_alpha_LR = LogisticRegression()\nreg_alpha_LR.fit(X, y, initial_w=np.array([1, 1, -1]), alpha = 10, max_epochs = steps, track_w=True)\n\n# train high alpha logistic regression\nhigh_alpha_LR = LogisticRegression()\nhigh_alpha_LR.fit(X, y, initial_w=np.array([1, 1, -1]), alpha = 20, max_epochs = steps, track_w=True)\n\nfig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 12))\n\n# figs 1 and 2\nfig1 = ax1.scatter(X[:,0], X[:,1], c = y)\nfig4 = ax4.scatter(X[:,0], X[:,1], c = y)\nfor i, (wi1, wi2) in enumerate(zip(reg_alpha_LR.w_history, high_alpha_LR.w_history)):\n    fig1 = draw_line(wi1, -3.5, 0, \"blue\", ax=ax1, alpha=((i+1+5)/(steps+5)))\n    fig4 = draw_line(wi2, -3.5, 0, \"red\", ax=ax4, alpha=((i+1+5)/(steps+5)))\n    \nax1.set(xlabel='Feauture 1', ylabel='Feauture 2', title=\"alpha=10\")\nax4.set(xlabel='Feauture 1', ylabel='Feauture 2', title=\"alpha=20\")\n\n# fig 2\nnum_steps = len(reg_alpha_LR.loss_history)\nax2.plot(np.arange(num_steps) + 1, reg_alpha_LR.loss_history, color=\"blue\")\nax2.set(xlabel='Epoch', ylabel='Loss')\n\n#fig 5\nnum_steps = len(high_alpha_LR.loss_history)\nax5.plot(np.arange(num_steps) + 1, high_alpha_LR.loss_history, color=\"red\")\nax5.set(xlabel='Epoch', ylabel='Loss')\n\n# fig 3\nnum_steps = len(reg_alpha_LR.score_history)\nax3.plot(np.arange(num_steps) + 1, reg_alpha_LR.score_history, color=\"blue\")\nax3.set(xlabel='Epoch', ylabel='Score')\n\n# fig 6\nnum_steps = len(high_alpha_LR.score_history)\nax6.plot(np.arange(num_steps) + 1, high_alpha_LR.score_history, color=\"red\")\nax6.set(xlabel='Epoch', ylabel='Score')\nNone\n\n\n\n\nIn the scatter plots above, I plot all of the best fit lines, increasing the opacity of the lines to 100% as we approach the final epoch. In the alpha=10 plot, we see that the line quickly finds roughly where it needs to be and then makes very small changes it approaches the optimal line. In the alpha=20 plot, the best fit lines are all over the place. We see why if we look at the loss graph. Every time the alpha=20 model gets close to a very low loss, it passes the minimum and the loss jumps back up again.\n\n\nExperiment 2 and 3: Batch Size and Momentum\nIn this experiment I will demonstrate how small batch gradient descent can boost the speed of an algorithm, and then how momentum can boost speed on top of that.\nAll models are trained with the same data, starting weights, and number of epochs.\n\nnp.random.seed(385325)\np_features = 6\nX, y = make_blobs(n_samples = 400, n_features = p_features - 1, centers = [(-1, 1, 0, -1, -1), (1, -1, 2, 1, 1)])\n\nsteps=250\n# train normal logistic regression\nreg_LR = LogisticRegression()\nreg_LR.fit(X, y, initial_w=[-1, 4, -4, 1, 3, -1], alpha = .01, max_epochs = steps)\n\n# train small batch logistic regression\nstochastic_LR = LogisticRegression()\nstochastic_LR.fit_stochastic(X, y, initial_w=[-1, 4, -4, 1, 3, -1], batch_size=25, alpha = .01, max_epochs = steps)\n\n# train small batch logistic regression with momentum\nstoch_momentum_LR = LogisticRegression()\nstoch_momentum_LR.fit_stochastic(X, y, initial_w=[-1, 4, -4, 1, 3, -1], momentum=.8, batch_size=25, alpha = .01, max_epochs = steps)\n\nfig, (ax2, ax3) = plt.subplots(1, 2, figsize=(18, 6))\n\n# fig 2\nnum_steps = len(reg_LR.loss_history)\nax2.plot(np.arange(num_steps) + 1, reg_LR.loss_history, label=\"regular gradient\")\nax2.plot(np.arange(num_steps) + 1, stochastic_LR.loss_history, label=\"stochastic\")\nax2.plot(np.arange(num_steps) + 1, stoch_momentum_LR.loss_history, label=\"stochastic w/momentum\")\nax2.set(xlabel='Epoch', ylabel='Loss')\nlegend = ax2.legend() \n\n# fig 3\nnum_steps = len(reg_LR.score_history)\nax3.plot(np.arange(num_steps) + 1, reg_LR.score_history, label=\"regular gradient\")\nax3.plot(np.arange(num_steps) + 1, stochastic_LR.score_history, label=\"stochastic\")\nax3.plot(np.arange(num_steps) + 1, stoch_momentum_LR.score_history, label=\"stochastic w/momentum\")\nax3.set(xlabel='Epoch', ylabel='Score')\nlegend = ax3.legend() \n\n\n\n\nThe data has five features, so I will have not shown any best fit lines. Instead, I charted the loss and score histories. These clearly show how the non sped up gradient descent has not even come close to reaching a minimum after 250 epochs. On the other hand, the stochastic descent took over 200 epochs, and the stochastic with momentum took only about 50."
  },
  {
    "objectID": "posts/final-project/index.html",
    "href": "posts/final-project/index.html",
    "title": "HAPI Prediction",
    "section": "",
    "text": "References\n\nAndjelkovic, Jovan, Branimir Ljubic, Ameen Abdel Hai, Marija Stanojevic, Martin Pavlovski, Wilson Diaz, and Zoran Obradovic. 2022. “Sequential Machine Learning in Prediction of Common Cancers.” Informatics in Medicine Unlocked 30: 100928. https://doi.org/https://doi.org/10.1016/j.imu.2022.100928.\n\n\nChe, Zhengping, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. 2018. “Recurrent Neural Networks for Multivariate Time Series with Missing Values.” Scientific Reports 8 (1): 6085. https://doi.org/10.1038/s41598-018-24271-9.\n\n\nGaspar, Susana, Miguel Peralta, Adilson Marques, Aglécia Budri, and Margarida Gaspar de Matos. 2019. “Effectiveness on Hospital-Acquired Pressure Ulcers Prevention: A Systematic Review.” International Wound Journal 16 (5): 1087–1102. https://doi.org/https://doi.org/10.1111/iwj.13147.\n\n\nKaewprag, Pacharmon, Cheryl Newton, Brenda Vermillion, Sookyung Hyun, Kun Huang, and Raghu Machiraju. 2017. “Predictive Models for Pressure Ulcers from Intensive Care Unit Electronic Health Records Using Bayesian Networks.” BMC Medical Informatics and Decision Making 17 (2): 65. https://doi.org/10.1186/s12911-017-0471-z.\n\n\nLevy, Joshua J., Jorge F. Lima, Megan W. Miller, Gary L. Freed, A. James O’Malley, and Rebecca T. Emeny. 2022. “Machine Learning Approaches for Hospital Acquired Pressure Injuries: A Retrospective Study of Electronic Medical Records.” Frontiers in Medical Technology 4. https://doi.org/10.3389/fmedt.2022.926667.\n\n\nLyder, Courtney H., Yun Wang, Mark Metersky, Maureen Curry, Rebecca Kliman, Nancy R. Verzier, and David R. Hunt. 2012. “Hospital-Acquired Pressure Ulcers: Results from the National Medicare Patient Safety Monitoring System Study.” Journal of the American Geriatrics Society 60 (9): 1603–8. https://doi.org/https://doi.org/10.1111/j.1532-5415.2012.04106.x.\n\n\nŠín, Petr, Alica Hokynková, Nováková Marie, Pokorná Andrea, Rostislav Krč, and Jan Podroužek. 2022. “Machine Learning-Based Pressure Ulcer Prediction in Modular Critical Care Data.” Diagnostics 12 (4). https://doi.org/10.3390/diagnostics12040850.\n\n\nSong, Jie, Yuan Gao, Pengbin Yin, Yi Li, Yang Li, Jie Zhang, Qingqing Su, Xiaojie Fu, and Hongying Pi. 2021. “The Random Forest Model Has the Best Accuracy Among the Four Pressure Ulcer Prediction Models Using Machine Learning Algorithms.” Risk Manag Healthc Policy 14 (March): 1175–87.\n\n\nWalther, Felix, Luise Heinrich, Jochen Schmitt, Maria Eberlein-Gonska, and Martin Roessler. 2022. “Prediction of Inpatient Pressure Ulcers Based on Routine Healthcare Data Using Machine Learning Methodology.” Scientific Reports 12 (1): 5044. https://doi.org/10.1038/s41598-022-09050-x.\n\n\nWells, B. J., K. M. Chagin, A. S. Nowacki, and M. W. Kattan. 2013. “Strategies for handling missing data in electronic health record derived data.” EGEMS (Wash DC) 1 (3): 1035."
  },
  {
    "objectID": "posts/gebru/index.html",
    "href": "posts/gebru/index.html",
    "title": "Dr. Timnit Gebru",
    "section": "",
    "text": "Dr. Timnit Gebru is a highly experienced and credentialed researcher who studies the social consequences of the advancement of artificial intelligence, and she will be delivering a talk on these issues to students at Middlebury College on April 24th. Dr. Gebru worked at Apple for many years before joining a lab under famed computer vision researcher Fei-Fei Li, at Stanford University. While there, her focus shifted more towards the sociological consequences of AI. After a few years at Stanford, she joined an AI ethics team at Google, ultimately leaving or being fired for writing a paper that some important people at Google very much did not like. Since then, she has continued to advocate for ethically focused AI. Her specific research focus appears to be in combating algorithmic bias. She was the second author on Joy Buolamwini’s famous paper that exposed how the best facial recognition software at the time performed significantly worse on Black women than White men.\n\n\nDr. Gebru opens her presentation by commenting on the extreme homogeneity of the machine learning community. She specifically comments on the underrepresentation of Black women, noting that often she was the only Black woman at conferences. She does highlight how the field of machine learning as a whole has improved (in part of course thanks to work by her and others), but she also points out that the computer vision subfield still lags behind.\nAs evidence for the importance of diversity, Dr. Gebru highlights how different people could have vastly different perspectives on the same technologies. She provides many examples of applications of computer vision that could be discriminatory. She points out how there has been progress in people realizing the importance of diverse datasets, but she also argues that unbiased algorithms can still do damage.\nShe talks about how, in search of diverse datasets, many people were included in datasets without their consent. She also talks about how people fail to consider how perfect classifiers can be used for dangerous purposes, so, even if a perfectly unbiased algorithm was possible, it would still be used to discriminate against marginalized groups. She also highlights how people naturally trust algorithms more than the accuracy of the model warrants. She advocates for the creation of a governmental oversight organization that would review AI applications for discrimination. She also stressed that even though AI may not be any more capable than a human on an individual task, the scale and efficiency at which it can do that task makes it much more dangerous (an example would be police using AI facial recognition vs human facial recognition).\nTLDR: The lack of diversity within the field of computer vision coupled with the dogged pursuit of classification accuracy has led people to create (and employ) algorithms that can accidentally (or on purpose) reinforce systemic oppression.\n\n\n\n\nWhat are your thoughts on content suggestion algorithms on apps and sites such as Tik Tok, Instagram, Twitter, and Reddit.\nDo you think OpenAIs efforts to control the outputs of ChatGPT have been sufficient?"
  },
  {
    "objectID": "posts/gebru/index.html#after-talk-reflection",
    "href": "posts/gebru/index.html#after-talk-reflection",
    "title": "Dr. Timnit Gebru",
    "section": "After talk reflection",
    "text": "After talk reflection\n\nTalk summary\nDr. Gebru started the talk with an introduction to the definition of eugenics that she was using. She informed us that eugenics was not an exclusively Nazi ideology, and that it outlasted them by many decades. She said it continues prominently today, albeit in a different form. Rather than the “negative” first wave eugenics of the Nazis, today we have “positive” second wave eugenics. Essentially, first wave eugenics involved getting rid of undesirables to improve the human stocks. On the other hand, second wave eugenics is about elevating those deemed to be fit. An example of this type of eugenics is encouraging smart people to reproduce (as a brief sidenote, Dr. Gebru also repeatedly singled out IQ tests as being a particularly common and also bogus tool for these second wave eugenicists).\nOnce the topic of eugenics had been introduced, she began to explain TESCREAL. She described how it has direct origins in eugenics, and how the eugenicist ideas are fundamental pillars of the respective ideologies. Elaborating more on TESCREAL, it is an acronym that catches many of the main types of people pushing for the creation of artificial general intelligence (AGI). The specifics of the categories don’t matter as much, and she didn’t get into all of them, but a couple she defined thoroughly for us are transhumanists, singularitarians, and cosmism (here is a thread elaborating on TESCREAL from a co-preeminent expert on the topic ). The common theme of TESCREALists is a desire to create a utopia through AGI, be it AI enhanced humans, or the AGI itself. Dr. Gebru characterizes this effort rather as a desire to create a new ruling class. Dr. Gebru folds into TESCREAL the people that vehemently oppose AGI for the threat it poses to humanity.\nAfter those definitions, the talk becomes more focused on what’s happening now, and less on the far (if you ask me) AGI future. A core component of Dr. Gebru’s critique of TESCREAL is that they claim to be the moral future of the world, and yet their current actions are immoral. Essentially, she says how can we trust the current leaders in AGI to lead us to a utopia when right now they are centralizing power to themselves, consuming all of the AI funding for AI research, exploiting and traumatizing workers, stealing from artists, and destroying the environment with high energy demands. At some point in the talk, she also stressed how the money focused incentive structure made positive AI research almost impossible.\nDr. Gebru finished her talk with a brief condemnation of the “pause” letter signed by many prominent tech figures that advocates for pausing AGI research while the risks can be more properly understood. She said that the signatories aren’t talking about the real current problems, but are instead distracting from the real problems with far off future ones. She also said that the letter places the blame for bad outcomes on the AI rather than the humans who are creating them.\nFrom there Dr. Gebru takes some questions. The first was from an Effective Altruist (and therefore a TESCREAList), but it was not really about AGI and was more about what is classified as second wave eugenics. The next question was about the actions of the mainstream media, and Dr. Gebru pointed out how the press just highlights and publicizes powerful people. She said when functioning properly the media should hold the powerful accountable, not just parrot their talking points. Next someone asked a question that caused me to cringe so hard I had to leave the room immediately. When I got back Dr. Gebru was talking about how cars have downsides and ultimately still benefit those in power and harm everyone else. After someone asked some AGI related question that I can’t remember the specifics of, Dr. Gebru took the opportunity to comment on the fact that AGI has no real definition and that real science should be tightly scoped. This was also the main theme for a few other later questions. The last question I will touch on was asking what AI researchers should do, and Dr. Gebru stressed the importance of collective action.\nTLDR: AGI is an undefined entity sought after by TESCREALists who view it as the path to utopia, but like other pursuits of utopia their pursuit is and will continue to be exclusionary and eugenicist.\n\n\nMy thoughts\nI thought the talk introduced some really interesting ideas about the dangers of dreaming of a utopia. Utopia and eugenics traditionally go hand in hand, and Dr. Gebru presents some compelling reasons that AGI utopias are inherently eugenicist. This talk led me to really consider whether certain ideas I hold are eugenicist, as I have had dreams of transhumanesque cybernetic upgrades for myself and others. Ultimately, I think that my views would not make me a eugenicist in Dr. Gebru’s eyes in the same way that she believes that not all gene editing is eugenics.\nI believe that Dr. Gebru did seem to be barely scratching the surface of what she had to say. She came across to me as generally thinking that the industrial revolution was a mistake (although I may be projecting). I believe this because of her comments on the perverse incentive structure for these companies and because of what she said about technology in general always exclusively benefitting the ruling class. In other words, many of her arguments against AGI also can be used against any other tech. I would love to read more from her on these topics. I think they have the potential to be incredibly interesting.\nI wish she would have talked about the effective altruists since I don’t really know anything about them other than the posters they put up around campus. Based solely off of the posters and their general mission statement, I would have thought that Dr. Gebru could be an effective altruist. Her point about how people in Ethiopia lack access to clean water seems to me like it belongs on one of those effective altruism posters. This paper argues that effective altruism has become too longterministic, but to me it doesn’t seem necessarily rotten in the same way as the utopia seekers of the TESCREAL bundle."
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Source code is here.\nIn this blog post I have implemented least-squares linear regression, which is linear regression using a least-squares cost function. Minimizing the least squares cost function actually has an analytical solution, which I have implemented in addition to gradient descent.\n\nfrom linreg import LinearRegression # your source code\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef draw_line(w, x_min, x_max, *, color=\"black\", ax=None, alpha=1, **kwargs):\n  x = np.linspace(x_min, x_max, 101)\n  if len(w) == 3:\n    y = -(w[0]*x + w[2])/w[1]\n  elif len(w) == 2:\n    y = w[0]*x + w[1]\n  if ax is None:\n    plt.plot(x, y, color = color, alpha=alpha, **kwargs)\n  else:\n    ax.plot(x, y, color = color, alpha=alpha, **kwargs)\n\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# train\nLR_analytical = LinearRegression()\nLR_analytical.fit_analytical(X_train, y_train) \nLR_gradient = LinearRegression()\nLR_gradient.fit_gradient(X_train, y_train, w=[.5, .5], max_steps=100, alpha=.005)\n\n# plot best fit lines\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\naxarr[0].scatter(X_train, y_train, color=\"gray\", alpha=.5, label=\"Train\", s=15)\naxarr[0].scatter(X_val, y_val, color=\"black\", alpha=1, label=\"Validation\", s=20)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\n\ndraw_line(LR_analytical.w, 0, 1, color=\"blue\",  ax=axarr[0], label=\"Analytical\", lw=\"2\")\ndraw_line(LR_gradient.w, 0, 1, color=\"red\", ax=axarr[0], label=\"Gradient\", linestyle=\"dotted\", lw=\"4\")\naxarr[0].legend()\n\n# plot score\naxarr[1].plot(LR_gradient.score_history)\nlabels = axarr[1].set(xlabel = \"Iteration\", ylabel = \"Score\", title = \"Score Through Training\")\naxarr[1].set_ylim([0, 1])\n\nplt.tight_layout()\n\nprint(\"\\nAnalytical method:\")\nprint(f\"Training score = {LR_gradient.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR_gradient.score(X_val, y_val).round(4)}\")\n\nprint(\"\\nGradient method:\")\nprint(f\"Training score = {LR_gradient.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR_gradient.score(X_val, y_val).round(4)}\")\n\n\nAnalytical method:\nTraining score = 0.4919\nValidation score = 0.4647\n\nGradient method:\nTraining score = 0.4919\nValidation score = 0.4647"
  },
  {
    "objectID": "posts/linear-regression/index.html#experiments",
    "href": "posts/linear-regression/index.html#experiments",
    "title": "Linear Regression",
    "section": "Experiments",
    "text": "Experiments\n\nExperiments 1 and 2: Many Features and LASSO Regularization\nIn this experiment we will increase the number of features up to n - 1 in order to study what happens to the training and validation scores.\nWe will also add use a LR model that adds a regularizing term to its loss function to fight against overfitting when the number of features is very high.\n\nfrom sklearn.linear_model import Lasso\n\nn_train = 100\nn_val = 100\nnoise = 0.2\nscores = []\nscores_lasso = []\nfor i in range(1, n_train):\n    p_features = i\n\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n    LR = LinearRegression()\n    LR.fit_analytical(X_train, y_train)     \n    \n    LR_lasso = Lasso(alpha = 0.001)\n    LR_lasso.fit(X_train, y_train)\n    \n    scores.append({\"train\": LR.score(X_train, y_train), \"validation\": LR.score(X_val, y_val)})\n    scores_lasso.append({\"train\": LR_lasso.score(X_train, y_train), \"validation\": LR_lasso.score(X_val, y_val)})\n\n# plot score\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n\nscores_df = pd.DataFrame(scores)\nscores_df.index = np.arange(1, len(scores_df) + 1)\nscores_df.plot(ax=ax0, xlabel=\"Number of features\", ylabel=\"Score\")\nax0.set_ylim([0, 1.05])\n\nscores_lasso_df = pd.DataFrame(scores_lasso)\nscores_lasso_df.index = np.arange(1, len(scores_lasso_df) + 1)\nscores_lasso_df.plot(ax=ax1, xlabel=\"Number of features\", ylabel=\"Score\")\n\nprint(f\"Scores with {n_train} training samples and {n_train-1} features:\")\nprint(f\"Training score = {round(scores[-1]['train'], 4)}\")\nprint(f\"Validation score = {round(scores[-1]['validation'], 4)}\")\n\nprint(f\"\\nScores while using modified loss function with regularization term:\")\nprint(f\"Training score = {round(scores_lasso[-1]['train'], 4)}\")\nprint(f\"Validation score = {round(scores_lasso[-1]['validation'], 4)}\")\n\nScores with 100 training samples and 99 features:\nTraining score = 1.0\nValidation score = 0.5331\n\nScores while using modified loss function with regularization term:\nTraining score = 0.9982\nValidation score = 0.8288\n\n\n\n\n\nAs we can clearly see, our implementation becomes severely overfit as the number of features approaches the number of training examples. The training score approaches near perfection, whereas the validation score gets worse.\nThe scikit-learn implementation with the regularization term also exhibits some pretty serious overfitting, but not to the same degree as our implementation. When the number of features is nearly equal to the number of training examples, the regularization term is able to keep the validation score from tanking.\n\n\nExperiment 3: Bike Share Data\nIn this experiment we will train a model to predict the number of riders of a bikesharing program in DC.\n\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\",\n        \"dteday\"]\n\nbikeshare = data[cols]\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\n\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\", \"dteday\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\", \"dteday\"], axis = 1)\ny_test = test[\"casual\"]\n\nLR = LinearRegression()\nLR.fit_analytical(X_train, y_train)\n\npreds_train = LR.predict(X_train)\npreds_test = LR.predict(X_test)\n\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(data['dteday']), data['casual'], label=\"Actual\")\nax.plot(pd.to_datetime(train['dteday']), preds_train, label=\"Training Set Predictions\")\nax.plot(pd.to_datetime(test['dteday']), preds_test, label=\"Test Set Predictions\")\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nax.legend()\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_test, y_test).round(4)}\")\n\nTraining score = 0.7318\nValidation score = 0.6968\n\n\n\n\n\n\nX_train\n\n\n\n\n\n  \n    \n      \n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      579\n      1\n      1\n      1\n      0.752500\n      0.659583\n      0.129354\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      580\n      2\n      1\n      1\n      0.765833\n      0.642500\n      0.215792\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      581\n      1\n      0\n      1\n      0.793333\n      0.613333\n      0.257458\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      582\n      1\n      0\n      1\n      0.769167\n      0.652500\n      0.290421\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      583\n      2\n      1\n      1\n      0.752500\n      0.654167\n      0.129354\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n584 rows × 18 columns\n\n\n\n\nprint(\"Weights for each feature\")\nfor i, (key, weight) in enumerate(zip(X_train.keys(), LR.w)):\n    print(f\" {i+1}. {key}: {weight}\")\n\nWeights for each feature\n 1. weathersit: -108.37113626599087\n 2. workingday: -791.6905491329867\n 3. yr: 280.58692732526924\n 4. temp: 1498.7151127165637\n 5. hum: -490.10033978027354\n 6. windspeed: -1242.8003807505336\n 7. holiday: -235.8793491765524\n 8. mnth_2: -3.3543971201747707\n 9. mnth_3: 369.2719555186909\n 10. mnth_4: 518.4087534451241\n 11. mnth_5: 537.3018861590382\n 12. mnth_6: 360.8079981484559\n 13. mnth_7: 228.88148124909466\n 14. mnth_8: 241.3164120150012\n 15. mnth_9: 371.50385386758205\n 16. mnth_10: 437.6008478681119\n 17. mnth_11: 252.43300404938137\n 18. mnth_12: 90.8214604976828\n\n\n\nExamining the weights:\nWe can see that our model found temperature to be the single largest factor in determining number of riders. It gave negative weights to weekdays and surprisingly holidays as well. The warmer months have higher weights than the winter months, but early spring and fall have higher weights than summer does. I believe that the temperature weight compensates for the lower weights of the summer months to make it so the model still predicts the highest ridership numbers during the summer. This makes sense, since every day during the summer in DC is hot… The year also was given a positive weight, reflecting the higher ridership numbers in 2012 compared to 2011."
  },
  {
    "objectID": "posts/auditing-allocative-bias/index.html",
    "href": "posts/auditing-allocative-bias/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "Source code is here.\nIn this blog post I create a model that predicts employment status from a few features such as age, sex, and disability status, and I then analyze it for racial bias. Notably, race was not one of the features used, but its effects as a feature can be mimicked from combinations of other features.\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\n\n# I chose to stay using Alabama, since they didn't seem to have DC. Roll Tide\nSTATE = \"AL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nI picked these features:\n\nSCHL - level of schooling\nMAR - marital status\nAGEP - age\nMIL - military experience\nDIS - disability status\nSEX\n\nAlso note, for RAC1P:\n\nWhite alone\nBlack or African American alone\nAmerican Indian alone\nAlaska Native alone\nAmerican Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races\nAsian alone\nNative Hawaiian and Other Pacific Islander alone\nSome Other Race alone\nTwo or More Races\n\nBut 4, 5, and 6 did not have enough examples, so I folded them into the Some Other Race category. The new indices are:\n\nWhite alone\nBlack or African American alone\nAmerican Indian alone\nAsian alone\nSome Other Race alone\nTwo or More Races\n\n\nfeature_categories = [\"SCHL\", \"MAR\", \"AGEP\", \"DIS\", \"SEX\", \"NATIVITY\"]\n\nEmploymentProblem = BasicProblem(\n    features=feature_categories,\n    target='ESR',\n    target_transform=lambda x: x == 1,  # 1 means employed\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n# fold some races into \"other\" category\ngroup[(group==4) | (group==5) | (group==7)] = 8\ngroup[group==6] = 4\ngroup[group==8] = 5\ngroup[group==9] = 6\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\ndf = pd.DataFrame(X_train, columns = feature_categories)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\n\n\nraces = [\n    \"White\",\n    \"Black\",\n    \"American Indian\",\n    \"Asian\",\n    \"Other Race\",\n    \"Two or More Races\"\n]\n\nprint(f\"1. There are {df.shape[0]} individuals in the dataset.\")\nprint(f\"2. Of those individuals, {df.label.sum()} are employed\")\nprint(f\"3, 4. Number and ratio employed by group:\")\n\nemployee_races = df[df[\"label\"] == True][\"group\"]\nemployed_per_group = employee_races.value_counts(sort=False)\nratio_employed_per_group = employed_per_group/df[\"group\"].value_counts(sort=False)\n\nprint(\"     Race\".ljust(28) + \"Employed\".rjust(10) + \"Ratio Employed\".rjust(20))\nfor index, race in enumerate(races):\n    line = f\"  {index+1}. {race}:\".ljust(28)\n    line += f\"{employed_per_group[index+1]} \".rjust(10)\n    line += f\"{round(ratio_employed_per_group[index+1] * 100, 2)}% \".rjust(20)\n    print(line)\n\n# plot disability race intersection\ngroup_dict = {i+1: status for i, status in enumerate(races)}\nworking_by_race = df.query(\"DIS==1\")[[\"label\", \"group\"]].value_counts(sort=False)\nfig, axarr = plt.subplots(1, 6, figsize=(15, 5))\nfor i, (index, race) in enumerate(group_dict.items()):\n    axarr[i].pie((working_by_race[False][index], working_by_race[True][index]), autopct='%1.1f%%')\n    axarr[i].set_title(race)\nplt.legend(labels=[\"Not employed\", \"Employed\"], loc=\"lower right\")\nprint(f\"5. A plot showing percent of those who have a disability who are employed, by race:\")\n\n1. There are 38221 individuals in the dataset.\n2. Of those individuals, 15638 are employed\n3, 4. Number and ratio employed by group:\n     Race                     Employed      Ratio Employed\n  1. White:                     12020              42.26% \n  2. Black:                      2976              36.88% \n  3. American Indian:              68              43.59% \n  4. Asian:                       226               50.0% \n  5. Other Race:                  157              35.76% \n  6. Two or More Races:           191              28.81% \n5. A plot showing percent of those who have a disability who are employed, by race:\n\n\n\n\n\n\n\n\n\nI performed a grid search with max_depth=[5, 10] and max_features=[10, 20, 50] and it found max_depth=5 and max_features=10 to be optimal.\n\n# parameters = {\"gradientboostingclassifier__max_depth\": (5, 10),\n#               \"gradientboostingclassifier__max_features\": (10, 20, 50)}\n# pipeline = make_pipeline(StandardScaler(), GradientBoostingClassifier())\n# clf = GridSearchCV(pipeline, \n#                    parameters, \n#                    scoring=\"accuracy\")\n# clf.fit(X_train, y_train)\n\npipeline = make_pipeline(StandardScaler(), GradientBoostingClassifier(max_depth=5, max_features=10))\nclf = pipeline.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\npipeline.get_params\n\n<bound method Pipeline.get_params of Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('gradientboostingclassifier',\n                 GradientBoostingClassifier(max_depth=5, max_features=10))])>\n\n\n\n\n\n\n\n\ndef compute_measures(*, y_test, y_pred, print_bool=False):\n    (TN, FP), (FN, TP) = confusion_matrix(y_test, y_pred)\n    data = {\n        \"Accuracy\": (y_pred == y_test).mean(),\n        \"PPV\": TP / (TP+FP),\n        \"FPR\": FP / (FP+TN),\n        \"FNR\": FN / (TP+FN)       \n    }\n\n    data[\"Predicted Employed\"] = (TP + FP) / (TP + FP + TN + FN)\n\n    if print_bool: \n        for key, value in data.items(): print(f\"{key}: {round(value * 100, 2)}%\")\n    return data\n\nprint(\"All data:\")\ncompute_measures(y_test=y_test, y_pred=y_pred, print_bool=True)\n\naudit_dict = {}\nfor index, race in group_dict.items():\n    # print(f\"\\n{race} subgroup:\")\n    audit_dict[race] = compute_measures(y_test=y_test[group_test==index], y_pred=y_pred[group_test==index])\npd.DataFrame(audit_dict).T\n\nAll data:\nAccuracy: 81.07%\nPPV: 76.35%\nFPR: 17.0%\nFNR: 21.69%\nPredicted Employed: 42.26%\n\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      PPV\n      FPR\n      FNR\n      Predicted Employed\n    \n  \n  \n    \n      White\n      0.809845\n      0.776081\n      0.169746\n      0.217306\n      0.432771\n    \n    \n      Black\n      0.814670\n      0.722981\n      0.170620\n      0.211382\n      0.393643\n    \n    \n      American Indian\n      0.769231\n      0.700000\n      0.187500\n      0.300000\n      0.384615\n    \n    \n      Asian\n      0.711538\n      0.690909\n      0.320755\n      0.254902\n      0.528846\n    \n    \n      Other Race\n      0.868687\n      0.864865\n      0.084746\n      0.200000\n      0.373737\n    \n    \n      Two or More Races\n      0.831395\n      0.666667\n      0.144000\n      0.234043\n      0.313953\n    \n  \n\n\n\n\n\nprint(\"Ground truth percentages employed for reference:\")\ntrue_employment_percentages = df.query('label==True')['group'].value_counts(sort=False) / df['group'].value_counts(sort=True)\nfor (index, race), percentage in zip(group_dict.items(), true_employment_percentages):\n    print(f\"{index}. {race}: {round(percentage*100, 2)}%\")\n\nGround truth percentages employed for reference:\n1. White: 42.26%\n2. Black: 36.88%\n3. American Indian: 43.59%\n4. Asian: 50.0%\n5. Other Race: 35.76%\n6. Two or More Races: 28.81%\n\n\n\n\n\nIs your model approximately calibrated?\nMy model should be calibrated. The score threshold for a positive prediction should be the same across all samples in the dataset.\nDoes your model satisfy approximate error rate balance?\nMy model does not fully satisfy error rate balance. The two largest groups (white and black) have similar FPR (~17%) and FNR (~22%), but many of the classes with lower prevalence have quite different rates.\nDoes your model satisfy statistical parity?\nMy model does not satisfy statistical parity. Predicted employment percentages vary over a range of about 20 percentage points.\n\n\n\n\n\n\nbias_df = pd.DataFrame(X_test, columns = feature_categories)\nbias_df[\"group\"] = group_test\nbias_df[\"label\"] = y_test\n\nintersectional_audit_dict = {}\nfor index, race in group_dict.items():\n    for subgroup in [1, 2]:\n        # print(f\"\\n{race} subgroup:\")\n        include_bool_arr = (group_test==index) | (bias_df[\"DIS\"] == subgroup)\n        intersectional_audit_dict[f\"With{['', 'out'][subgroup-1]} Disability, {race}\"] = compute_measures(\n            y_test=y_test[include_bool_arr], \n            y_pred=y_pred[include_bool_arr]\n        )\npd.DataFrame(intersectional_audit_dict).T\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      PPV\n      FPR\n      FNR\n      Predicted Employed\n    \n  \n  \n    \n      With Disability, White\n      0.812352\n      0.774298\n      0.156832\n      0.231657\n      0.408683\n    \n    \n      Without Disability, White\n      0.808649\n      0.764808\n      0.181000\n      0.205327\n      0.442121\n    \n    \n      With Disability, Black\n      0.825578\n      0.716609\n      0.099673\n      0.363261\n      0.251975\n    \n    \n      Without Disability, Black\n      0.805474\n      0.765445\n      0.208575\n      0.177550\n      0.486559\n    \n    \n      With Disability, American Indian\n      0.844875\n      0.623377\n      0.019256\n      0.839465\n      0.042659\n    \n    \n      Without Disability, American Indian\n      0.802623\n      0.766054\n      0.224994\n      0.166118\n      0.510608\n    \n    \n      With Disability, Asian\n      0.838470\n      0.636364\n      0.028479\n      0.771513\n      0.064293\n    \n    \n      Without Disability, Asian\n      0.802648\n      0.766365\n      0.224861\n      0.166256\n      0.510671\n    \n    \n      With Disability, Other Race\n      0.847304\n      0.699029\n      0.020052\n      0.779817\n      0.054992\n    \n    \n      Without Disability, Other Race\n      0.802621\n      0.766113\n      0.224831\n      0.166301\n      0.510409\n    \n    \n      With Disability, Two or More Races\n      0.844525\n      0.638655\n      0.026841\n      0.772455\n      0.061467\n    \n    \n      Without Disability, Two or More Races\n      0.802746\n      0.765920\n      0.224529\n      0.166301\n      0.509882\n    \n  \n\n\n\n\nThe false negative rates here are really interesting. It seems that the model does not significantly alter its prediction based off disability for white people, but for every other racial category the changes are quite large. These numbers almost make sense when you look at the above pie plots: Asian’s and American Indian’s with disabilities are twice as likely to be working. This would partially explain why their false negative rates are high, but it wouldn’t explain the other races with very high FNRs.\n\n\n\nfor feat, importance in zip(feature_categories, clf[1].feature_importances_):\n    print(f\"{feat}:, {round(importance*100, 2)}%\")\n\nSCHL:, 36.14%\nMAR:, 2.65%\nAGEP:, 46.12%\nDIS:, 11.35%\nSEX:, 3.1%\nNATIVITY:, 0.64%\n\n\n\n\n\n\n\nA model trained to predict unemployment could be used for a variety of purposes. Here are two examples where it could be used:\n\nIt could be used to direct job ads to people that are unemployed, hoping to find people that are looking for work.\nIt could be used to direct job ads to only people that are employed, viewing the unemployed as undesirable.\n\nIn this sense, depending on its application, it could both help and hurt unemployed people. If we take American Indians as an example, since my model only had a very low FPR for them, they would be less likely to be excluded from job ads targeted to unemployed people, and more likely to be excluded from ads targeted to employed people.\nThe actual real world applications for my model specifically are slim. For starters, it is a very mediocre classifier, but, also, a model trained to predict employment status from the identifiers that I used could only be used by somebody with access to a bizarre dataset. The features I used are generally not easier to come by than employment status is to gain directly. Furthermore, the features that are harder to acquire are the more important ones. Perhaps Instagram or Facebook could analyze your account to come up with age, sex, and relationship status, but level of schooling and disability seem much harder, and for little payoff. To put it plainly, anybody that really needs employment status usually has the ability to ask directly, and if they can’t ask then they’re probably not going to have the data necessary to predict it.\nI think my model does display some problematic bias. The most concerning issue to me is the error rate balance. More specifically, I am concerned by how high the FPR is for American Indians and how high the FNR is for Asians. Most of the other values are relatively in line with each other, but these two stand out. If this model were to be implemented it, then American Indians and Asians would not be properly represented.\nBeyond concerns of bias, I think my model is incredibly innocuous. A model that predicted employment status through gait or clothing would be much more dangerous since it could be employed so much wider. The issue of acquiring the data necessary makes my model highly impractical to actually use by someone with malicious intentions."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Experiment 1: Linearly Separable 2D Data\nThis experiment looks at linearly seperable data with 2 features. The perceptron is able to quickly find a line that achieves 100% accuracy.\n\nnp.random.seed(12345)\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.8, -1.9), (1.6, 1.7)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 2: NOT Linearly Separable 2D Data\nThis experiment looks at data with 2 features that is not linearly seperable. The perceptron is not able to find a set of weights (a line) that can accuractely predict all of the data points because the data because this data is not linearly seperable.\nAlso, one of the qualities of the perceptron is that it does not converge, and we can see in the graph below containing the accuracies over the iterations that the accuracy continues to jump up and down until it reaches the max number of steps.\n\nnp.random.seed(54321)\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 3: Linearly Separable (?) 5D Data\nThis data containing 5 features turns out to be linearly seperable. We can tell it is linearly seperable without visualizing it because the perceptron is able to find line weights that make predictions with an accuracy of 100%.\n\nnp.random.seed(12321)\nn = 100\np_features = 6\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.75, -1.75), (1.75, 1.75)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThis data is linearly seperable. The perceptron was able to achieve 100% accuracy!\n\n\nRuntime Complexity of the Update Step\nThe update contains multiple individual steps occuring in sequence. We will look at them all individually\nFirst lets look at making the prediction: np.sign(np.dot(self.w, X_[i])). This operation does a 1xp dot px1 operation. This specific step is O(p).\nNext, lets look at the forming of the gate: (y_[i] * y_pred) < 0. This is clearly O(1).\nNow for the update: self.w + false_check * (y_[i] * X_[i]). The major component here is the scalar * vector operation. Since the vector is of length p, this is O(p) as well.\nWe could end there and be done, but we check the overall accuracy as part of the update step: (np.dot(self.w, X_.T) > 0). This is a dot product operation of 1xp vector and a pxn matrix. Therefore, this step has a runtime of O(pn).\nSince all of the steps are done in sequence, the total necessary runtime for the update step is O(p), and the total runtime for my implementation is O(pn)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Final Poject: HAPI prediction.\n\n\n\n\n\n\nMay 23, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #8: Dr. Timnit Gebru\n\n\n\n\n\n\nApr 18, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #5: auditing allocative bias.\n\n\n\n\n\n\nApr 11, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #4: linear regression.\n\n\n\n\n\n\nApr 8, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #2: logistic regression.\n\n\n\n\n\n\nMar 29, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #3: kernel logistic regression.\n\n\n\n\n\n\nMar 8, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #1: perceptrons.\n\n\n\n\n\n\nFeb 20, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog!"
  }
]