[
  {
    "objectID": "posts/kernel-logistic-regression/index.html",
    "href": "posts/kernel-logistic-regression/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Source code is here.\nIn this blog post I have implemented kernel logistic regression. Kernel logistic regression is a lot like regular linear logistic regression except we modify the data beforehand in order to still be able to use linear regression on nonlinear data.\nFor those that are curious, my loss function remains largely unchanged. I simply use the kernel matrix instead of the regular nxp data matrix. I use all numpy functions that performs the operations on all of the rows.\n\nfrom kernel_logreg import KernelLogisticRegression\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\nfrom scipy.optimize import minimize\nfrom sklearn.datasets import make_blobs, make_circles, make_moons\nfrom sklearn.metrics.pairwise import rbf_kernel\n\n\nnp.seterr(all='ignore')\n\ndef draw_line(w, x_min, x_max, color=\"black\", ax=None, alpha=1):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    if ax is None:\n        plt.plot(x, y, color = color, alpha=alpha)\n    else:\n        ax.plot(x, y, color = color, alpha=alpha)\n\ndef construct_plot(X, y, clf, ax=None, title=\"Accuracy = \"):\n    plot_decision_regions(X, y, clf=clf, ax=ax)\n    if ax is None: ax = plt.gca()\n    title = ax.set(title = f\"{title}{(KLR.predict(X) == y).mean()}\",\n                        xlabel = \"Feature 1\", \n                        ylabel = \"Feature 2\")"
  },
  {
    "objectID": "posts/kernel-logistic-regression/index.html#experiments",
    "href": "posts/kernel-logistic-regression/index.html#experiments",
    "title": "Kernel Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\n\nExperiment 1: Basic Check\nHere I am just confirming that all of my code actually works.\nRunning a test on moon shaped data with gamma=0.1 I get a model that is a pretty good fit for both train and test data.\n\nX_train, y_train = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X_train, y_train)\n\nX_test, y_test = make_moons(100, shuffle = True, noise = 0.2)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\nconstruct_plot(X_train, y_train, KLR, ax1, \"Training Accuracy = \")\nconstruct_plot(X_test, y_test, KLR, ax2, \"Test Accuracy = \")\n\n\n\n\n\n\nExperiment 2: Choosing Gamma\nIn this experiment I will demonstrate the importance of choosing a good gamma value. A gamma too low will underfit the data, but a gamma too high will overfit the data. As we can see values for gamma above 1 begin to lose accuracy on the test set.\n\nX_train, y_train = make_moons(80, shuffle = True, noise = 0.2)\nX_test, y_test = make_moons(80, shuffle = True, noise = 0.2)\ngammas = [.01, .1, 1, 10, 100, 1000, 10000]\nfig, axarr = plt.subplots(len(gammas), 2, figsize=(12, 6*len(gammas)))\n\nfor i, gamma in enumerate(gammas):\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"gamma={gamma} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"gamma={gamma} Test Accuracy = \")\n\n\n\n\n\n\nExperiment 3: Varied Noise\nIn this experiment I vary the noise paramater of the make_moons function, and I test what values of gamma are best with increased noise.\nIn the last experiment we tested with noise=.2 and saw that a gamma value of around 1 was sufficient.\n\nTesting with Noise=.05\nFrom this test we can gather that when there is very little noise (and very little difference between the train and test data), we just need a high enough gamma to follow the shape.\n\nX_train, y_train = make_moons(80, shuffle = True, noise = 0.05)\nX_test, y_test = make_moons(80, shuffle = True, noise = 0.05)\ngammas = [.01, .1, 1, 10]\nfig, axarr = plt.subplots(len(gammas), 2, figsize=(12, 6*len(gammas)))\n\nfor i, gamma in enumerate(gammas):\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"gamma={gamma} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"gamma={gamma} Test Accuracy = \")\n\n\n\n\n\n\nTesting with Noise=.04\nIn this test we see that a gamma of .1 does a the best at matching the shape we want. Higher than .1 is overfit and below .1 is underfit.\n\nX_train, y_train = make_moons(80, shuffle = True, noise = 0.4)\nX_test, y_test = make_moons(80, shuffle = True, noise = 0.4)\ngammas = [.01, .1, 1, 10]\nfig, axarr = plt.subplots(len(gammas), 2, figsize=(12, 6*len(gammas)))\n\nfor i, gamma in enumerate(gammas):\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"gamma={gamma} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"gamma={gamma} Test Accuracy = \")\n\n\n\n\n\n\nConclusions from Experiment 3\nOverall, I don’t have enough evidence to suggest that the best value for gamma depends on the noise level. This generally matches what I have seen in the documentation that assigns gamma based off of the size of the training set.\n\n\n\nExperiment 4: Other Shapes\n\nTesting with gamma=.1\n\nnoises = [0, .1, .2]\nfig, axarr = plt.subplots(len(noises), 2, figsize=(12, 6*len(noises)))\ngamma = .1\nfor i, noise in enumerate(noises):\n    X_train, y_train = make_circles(50, shuffle = True, noise = noise)\n    X_test, y_test = make_circles(50, shuffle = True, noise = noise)\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"noise={noise} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"noise={noise} Test Accuracy = \")\n\n\n\n\n\n\nTesting with gamma=1\n\nnoises = [0, .1, .2]\nfig, axarr = plt.subplots(len(noises), 2, figsize=(12, 6*len(noises)))\ngamma = 1\nfor i, noise in enumerate(noises):\n    X_train, y_train = make_circles(50, shuffle = True, noise = noise)\n    X_test, y_test = make_circles(50, shuffle = True, noise = noise)\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"noise={noise} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"noise={noise} Test Accuracy = \")\n\n\n\n\n\n\nTesting with gamma=10\nA gamma value of 10 is too high with\n\nnoises = [0, .1, .2]\nfig, axarr = plt.subplots(len(noises), 2, figsize=(12, 6*len(noises)))\ngamma = 10\nfor i, noise in enumerate(noises):\n    X_train, y_train = make_circles(50, shuffle = True, noise = noise)\n    X_test, y_test = make_circles(50, shuffle = True, noise = noise)\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"noise={noise} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"noise={noise} Test Accuracy = \")\n\n\n\n\n\n\nConclusions from Experiment 4\nWhen there is no noise pretty much any reasonable gamma will do, but once there starts to be signficant noise, the lower gamma value does better on this set."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code is here.\nIn this blog post I have implemented logistic regression.\n\nfrom logreg import LogisticRegression\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ndef draw_line(w, x_min, x_max, color=\"black\", ax=None, alpha=1):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  if ax is None:\n    plt.plot(x, y, color = color, alpha=alpha)\n  else:\n    ax.plot(x, y, color = color, alpha=alpha)"
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiments",
    "href": "posts/logistic-regression/index.html#experiments",
    "title": "Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\n\nExperiment 1: Alpha Too Large\nIn this experiment I train two models on the same data with the same initial weights and the same number of epochs in order to demonstrate what can happen when alpha is too high.\nIn the charts and graphs below, the blue corresponds to the model trained with alpha=10 and the red corresponds with alpha=20.\nIn the scatter plots, I plot all of the best fit lines, increasing the opacity of the lines to 100% as we approach the final epoch. In the alpha=10 plot, we see that the line quickly finds roughly where it needs to be and then makes very small changes it approaches the optimal line. In the alpha=20 plot, the best fit lines are all over the place. We see why if we look at the loss graph. Every time the alpha=20 model gets close to a very low loss, it passes the minimum and the loss jumps back up again.\n\nnp.random.seed(38532555)\np_features = 3\nX, y = make_blobs(n_samples = 80, n_features = p_features - 1, centers = [(-1.5, -1.5), (-2, 1.5)])\n\nsteps=300\n# train normal logistic regression\nreg_alpha_LR = LogisticRegression()\nreg_alpha_LR.fit(X, y, initial_w=np.array([1, 1, -1]), alpha = 10, max_epochs = steps, track_w=True)\n\n# train high alpha logistic regression\nhigh_alpha_LR = LogisticRegression()\nhigh_alpha_LR.fit(X, y, initial_w=np.array([1, 1, -1]), alpha = 20, max_epochs = steps, track_w=True)\n\nfig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 12))\n\n# figs 1 and 2\nfig1 = ax1.scatter(X[:,0], X[:,1], c = y)\nfig4 = ax4.scatter(X[:,0], X[:,1], c = y)\nfor i, (wi1, wi2) in enumerate(zip(reg_alpha_LR.w_history, high_alpha_LR.w_history)):\n    fig1 = draw_line(wi1, -3.5, 0, \"blue\", ax=ax1, alpha=((i+1+5)/(steps+5)))\n    fig4 = draw_line(wi2, -3.5, 0, \"red\", ax=ax4, alpha=((i+1+5)/(steps+5)))\n    \nax1.set(xlabel='Feauture 1', ylabel='Feauture 2', title=\"alpha=10\")\nax4.set(xlabel='Feauture 1', ylabel='Feauture 2', title=\"alpha=20\")\n\n# fig 2\nnum_steps = len(reg_alpha_LR.loss_history)\nax2.plot(np.arange(num_steps) + 1, reg_alpha_LR.loss_history, color=\"blue\")\nax2.set(xlabel='Epoch', ylabel='Loss')\n\n#fig 5\nnum_steps = len(high_alpha_LR.loss_history)\nax5.plot(np.arange(num_steps) + 1, high_alpha_LR.loss_history, color=\"red\")\nax5.set(xlabel='Epoch', ylabel='Loss')\n\n# fig 3\nnum_steps = len(reg_alpha_LR.score_history)\nax3.plot(np.arange(num_steps) + 1, reg_alpha_LR.score_history, color=\"blue\")\nax3.set(xlabel='Epoch', ylabel='Score')\n\n# fig 6\nnum_steps = len(high_alpha_LR.score_history)\nax6.plot(np.arange(num_steps) + 1, high_alpha_LR.score_history, color=\"red\")\nax6.set(xlabel='Epoch', ylabel='Score')\nNone\n\n\n\n\n\n\nExperiment 2 and 3: Batch Size and Momentum\nIn this experiment I will demonstrate how small batch gradient descent can boost the speed of an algorithm, and then how momentum can boost speed on top of that.\nAll models are trained with the same data, starting weights, and number of epochs.\nThe data has five features, so I will not be showing any best fit lines. Instead, I chart the loss and score histories. These cleary show how the non sped up gradient descent has not even come close to reaching a minimum after 250 epochs. The stochasitc descent took over 200 epochs, but the stochastic with momentum took only about 50.\n\nnp.random.seed(385325)\np_features = 6\nX, y = make_blobs(n_samples = 400, n_features = p_features - 1, centers = [(-1, 1, 0, -1, -1), (1, -1, 2, 1, 1)])\n\nsteps=250\n# train normal logistic regression\nreg_LR = LogisticRegression()\nreg_LR.fit(X, y, initial_w=[-1, 4, -4, 1, 3, -1], alpha = .01, max_epochs = steps)\n\n# train small batch logistic regression\nstochastic_LR = LogisticRegression()\nstochastic_LR.fit_stochastic(X, y, initial_w=[-1, 4, -4, 1, 3, -1], batch_size=25, alpha = .01, max_epochs = steps)\n\n# train small batch logistic regression with momentum\nstoch_momentum_LR = LogisticRegression()\nstoch_momentum_LR.fit_stochastic(X, y, initial_w=[-1, 4, -4, 1, 3, -1], momentum=.8, batch_size=25, alpha = .01, max_epochs = steps)\n\nfig, (ax2, ax3) = plt.subplots(1, 2, figsize=(18, 6))\n\n# fig 2\nnum_steps = len(reg_LR.loss_history)\nax2.plot(np.arange(num_steps) + 1, reg_LR.loss_history, label=\"regular gradient\")\nax2.plot(np.arange(num_steps) + 1, stochastic_LR.loss_history, label=\"stochastic\")\nax2.plot(np.arange(num_steps) + 1, stoch_momentum_LR.loss_history, label=\"stochastic w/momentum\")\nax2.set(xlabel='Epoch', ylabel='Loss')\nlegend = ax2.legend() \n\n# fig 3\nnum_steps = len(reg_LR.score_history)\nax3.plot(np.arange(num_steps) + 1, reg_LR.score_history, label=\"regular gradient\")\nax3.plot(np.arange(num_steps) + 1, stochastic_LR.score_history, label=\"stochastic\")\nax3.plot(np.arange(num_steps) + 1, stoch_momentum_LR.score_history, label=\"stochastic w/momentum\")\nax3.set(xlabel='Epoch', ylabel='Score')\nlegend = ax3.legend()"
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Source code is here.\nIn this blog post I have implemented least-squares linear regression, which is linear regression using a least-squares cost function. Minimizing the least squares cost function actually has an analytical solution, which I have implemented in addition to gradient descent.\n\nfrom linreg import LinearRegression # your source code\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef draw_line(w, x_min, x_max, *, color=\"black\", ax=None, alpha=1, **kwargs):\n  x = np.linspace(x_min, x_max, 101)\n  if len(w) == 3:\n    y = -(w[0]*x + w[2])/w[1]\n  elif len(w) == 2:\n    y = w[0]*x + w[1]\n  if ax is None:\n    plt.plot(x, y, color = color, alpha=alpha, **kwargs)\n  else:\n    ax.plot(x, y, color = color, alpha=alpha, **kwargs)\n\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# train\nLR_analytical = LinearRegression()\nLR_analytical.fit_analytical(X_train, y_train) \nLR_gradient = LinearRegression()\nLR_gradient.fit_gradient(X_train, y_train, w=[.5, .5], max_steps=100, alpha=.005)\n\n# plot best fit lines\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\naxarr[0].scatter(X_train, y_train, color=\"gray\", alpha=.5, label=\"Train\", s=15)\naxarr[0].scatter(X_val, y_val, color=\"black\", alpha=1, label=\"Validation\", s=20)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\n\ndraw_line(LR_analytical.w, 0, 1, color=\"blue\",  ax=axarr[0], label=\"Analytical\", lw=\"2\")\ndraw_line(LR_gradient.w, 0, 1, color=\"red\", ax=axarr[0], label=\"Gradient\", linestyle=\"dotted\", lw=\"4\")\naxarr[0].legend()\n\n# plot score\naxarr[1].plot(LR_gradient.score_history)\nlabels = axarr[1].set(xlabel = \"Iteration\", ylabel = \"Score\", title = \"Score Through Training\")\naxarr[1].set_ylim([0, 1])\n\nplt.tight_layout()\n\nprint(\"\\nAnalytical method:\")\nprint(f\"Training score = {LR_gradient.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR_gradient.score(X_val, y_val).round(4)}\")\n\nprint(\"\\nGradient method:\")\nprint(f\"Training score = {LR_gradient.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR_gradient.score(X_val, y_val).round(4)}\")\n\n\nAnalytical method:\nTraining score = 0.6929\nValidation score = 0.6476\n\nGradient method:\nTraining score = 0.6929\nValidation score = 0.6476"
  },
  {
    "objectID": "posts/linear-regression/index.html#experiments",
    "href": "posts/linear-regression/index.html#experiments",
    "title": "Linear Regression",
    "section": "Experiments",
    "text": "Experiments\n\nExperiments 1 and 2: Many Features and LASSO Regularization\nIn this experiment we will increase the number of features up to n - 1 in order to study what happens to the training and validation scores.\nWe will also add use a LR model that adds a regularizing term to its loss function to fight against overfitting when the number of features is very high.\n\nfrom sklearn.linear_model import Lasso\n\nn_train = 100\nn_val = 100\nnoise = 0.2\nscores = []\nscores_lasso = []\nfor i in range(1, n_train):\n    p_features = i\n\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n    LR = LinearRegression()\n    LR.fit_analytical(X_train, y_train)     \n    \n    LR_lasso = Lasso(alpha = 0.001)\n    LR_lasso.fit(X_train, y_train)\n    \n    scores.append({\"train\": LR.score(X_train, y_train), \"validation\": LR.score(X_val, y_val)})\n    scores_lasso.append({\"train\": LR_lasso.score(X_train, y_train), \"validation\": LR_lasso.score(X_val, y_val)})\n\n# plot score\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n\nscores_df = pd.DataFrame(scores)\nscores_df.index = np.arange(1, len(scores_df) + 1)\nscores_df.plot(ax=ax0, xlabel=\"Number of features\", ylabel=\"Score\")\nax0.set_ylim([0, 1.05])\n\nscores_lasso_df = pd.DataFrame(scores_lasso)\nscores_lasso_df.index = np.arange(1, len(scores_lasso_df) + 1)\nscores_lasso_df.plot(ax=ax1, xlabel=\"Number of features\", ylabel=\"Score\")\n\nprint(f\"Scores with {n_train} training samples and {n_train-1} features:\")\nprint(f\"Training score = {round(scores[-1]['train'], 4)}\")\nprint(f\"Validation score = {round(scores[-1]['validation'], 4)}\")\n\nprint(f\"\\nScores while using modified loss function with regularization term:\")\nprint(f\"Training score = {round(scores_lasso[-1]['train'], 4)}\")\nprint(f\"Validation score = {round(scores_lasso[-1]['validation'], 4)}\")\n\nScores with 100 training samples and 99 features:\nTraining score = 1.0\nValidation score = -2269.1898\n\nScores while using modified loss function with regularization term:\nTraining score = 0.9973\nValidation score = 0.8407\n\n\n\n\n\nAs we can clearly see, our implementation becomes severely overfit as the number of features approaches the number of training examples. The training score approaches near perfection, whereas the validation score gets worse.\nThe scikit-learn implementation with the regularization term also exhibits some pretty serious overfitting, but not to the same degree as our implementation. When the number of features is nearly equal to the number of training examples, the regularization term is able to keep the validation score from tanking.\n\n\nExperiment 3: Bike Share Data\nIn this experiment we will train a model to predict the number of riders of a bikesharing program in DC.\n\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\",\n        \"dteday\"]\n\nbikeshare = data[cols]\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\n\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\", \"dteday\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\", \"dteday\"], axis = 1)\ny_test = test[\"casual\"]\n\nLR = LinearRegression()\nLR.fit_analytical(X_train, y_train)\n\npreds_train = LR.predict(X_train)\npreds_test = LR.predict(X_test)\n\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(data['dteday']), data['casual'], label=\"Actual\")\nax.plot(pd.to_datetime(train['dteday']), preds_train, label=\"Training Set Predictions\")\nax.plot(pd.to_datetime(test['dteday']), preds_test, label=\"Test Set Predictions\")\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nax.legend()\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_test, y_test).round(4)}\")\n\nTraining score = 0.7318\nValidation score = 0.6968\n\n\n\n\n\n\nX_train\n\n\n\n\n\n  \n    \n      \n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      579\n      1\n      1\n      1\n      0.752500\n      0.659583\n      0.129354\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      580\n      2\n      1\n      1\n      0.765833\n      0.642500\n      0.215792\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      581\n      1\n      0\n      1\n      0.793333\n      0.613333\n      0.257458\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      582\n      1\n      0\n      1\n      0.769167\n      0.652500\n      0.290421\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      583\n      2\n      1\n      1\n      0.752500\n      0.654167\n      0.129354\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n584 rows × 18 columns\n\n\n\n\nprint(\"Weights for each feature\")\nfor i, (key, weight) in enumerate(zip(X_train.keys(), LR.w)):\n    print(f\" {i+1}. {key}: {weight}\")\n\nWeights for each feature\n 1. weathersit: -108.37113626599087\n 2. workingday: -791.6905491329867\n 3. yr: 280.58692732526924\n 4. temp: 1498.7151127165637\n 5. hum: -490.10033978027354\n 6. windspeed: -1242.8003807505336\n 7. holiday: -235.8793491765524\n 8. mnth_2: -3.3543971201747707\n 9. mnth_3: 369.2719555186909\n 10. mnth_4: 518.4087534451241\n 11. mnth_5: 537.3018861590382\n 12. mnth_6: 360.8079981484559\n 13. mnth_7: 228.88148124909466\n 14. mnth_8: 241.3164120150012\n 15. mnth_9: 371.50385386758205\n 16. mnth_10: 437.6008478681119\n 17. mnth_11: 252.43300404938137\n 18. mnth_12: 90.8214604976828\n\n\n\nExamining the weights:\nWe can see that our model found temperature to be the single largest factor in determining number of riders. It gave negative weights to weekdays and surprisingly holidays as well. The warmer months have higher weights than the winter months, but early spring and fall have higher weights than summer does. I believe that the temperature weight compensates for the lower weights of the summer months to make it so the model still predicts the highest ridership numbers during the summer. This makes sense, since every day during the summer in DC is hot… The year also was given a positive weight, reflecting the higher ridership numbers in 2012 compared to 2011."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Experiment 1: Linearly Separable 2D Data\nThis experiment looks at linearly seperable data with 2 features. The perceptron is able to quickly find a line that achieves 100% accuracy.\n\nnp.random.seed(12345)\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.8, -1.9), (1.6, 1.7)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 2: NOT Linearly Separable 2D Data\nThis experiment looks at data with 2 features that is not linearly seperable. The perceptron is not able to find a set of weights (a line) that can accuractely predict all of the data points because the data because this data is not linearly seperable.\nAlso, one of the qualities of the perceptron is that it does not converge, and we can see in the graph below containing the accuracies over the iterations that the accuracy continues to jump up and down until it reaches the max number of steps.\n\nnp.random.seed(54321)\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 3: Linearly Separable (?) 5D Data\nThis data containing 5 features turns out to be linearly seperable. We can tell it is linearly seperable without visualizing it because the perceptron is able to find line weights that make predictions with an accuracy of 100%.\n\nnp.random.seed(12321)\nn = 100\np_features = 6\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.75, -1.75), (1.75, 1.75)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThis data is linearly seperable. The perceptron was able to achieve 100% accuracy!\n\n\nRuntime Complexity of the Update Step\nThe update contains multiple individual steps occuring in sequence. We will look at them all individually\nFirst lets look at making the prediction: np.sign(np.dot(self.w, X_[i])). This operation does a 1xp dot px1 operation. This specific step is O(p).\nNext, lets look at the forming of the gate: (y_[i] * y_pred) < 0. This is clearly O(1).\nNow for the update: self.w + false_check * (y_[i] * X_[i]). The major component here is the scalar * vector operation. Since the vector is of length p, this is O(p) as well.\nWe could end there and be done, but we check the overall accuracy as part of the update step: (np.dot(self.w, X_.T) > 0). This is a dot product operation of 1xp vector and a pxn matrix. Therefore, this step has a runtime of O(pn).\nSince all of the steps are done in sequence, the runtime for the total update step is O(pn)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog post #4: linear regression.\n\n\n\n\n\n\nApr 8, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #2: logistic regression.\n\n\n\n\n\n\nMar 29, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #3: kernel logistic regression.\n\n\n\n\n\n\nMar 8, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #1: perceptrons.\n\n\n\n\n\n\nFeb 20, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog!"
  }
]