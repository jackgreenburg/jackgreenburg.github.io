[
  {
    "objectID": "posts/kernel-logistic-regression/index.html",
    "href": "posts/kernel-logistic-regression/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Source code is here.\nIn this blog post I have implemented kernel logistic regression. Kernel logistic regression is a lot like regular linear logistic regression except we modify the data beforehand in order to still be able to use linear regression on nonlinear data.\nFor those that are curious, my loss function remains largely unchanged. I simply use the kernel matrix instead of the regular nxp data matrix. I use all numpy functions that performs the operations on all of the rows.\n\nfrom kernel_logreg import KernelLogisticRegression\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\nfrom scipy.optimize import minimize\nfrom sklearn.datasets import make_blobs, make_circles, make_moons\nfrom sklearn.metrics.pairwise import rbf_kernel\n\n\nnp.seterr(all='ignore')\n\ndef draw_line(w, x_min, x_max, color=\"black\", ax=None, alpha=1):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    if ax is None:\n        plt.plot(x, y, color = color, alpha=alpha)\n    else:\n        ax.plot(x, y, color = color, alpha=alpha)\n\ndef construct_plot(X, y, clf, ax=None, title=\"Accuracy = \"):\n    plot_decision_regions(X, y, clf=clf, ax=ax)\n    if ax is None: ax = plt.gca()\n    title = ax.set(title = f\"{title}{(KLR.predict(X) == y).mean()}\",\n                        xlabel = \"Feature 1\", \n                        ylabel = \"Feature 2\")"
  },
  {
    "objectID": "posts/kernel-logistic-regression/index.html#experiments",
    "href": "posts/kernel-logistic-regression/index.html#experiments",
    "title": "Kernel Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\n\nExperiment 1: Basic Check\nHere I am just confirming that all of my code actually works.\nRunning a test on moon shaped data with gamma=0.1 I get a model that is a pretty good fit for both train and test data.\n\nX_train, y_train = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X_train, y_train)\n\nX_test, y_test = make_moons(100, shuffle = True, noise = 0.2)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\nconstruct_plot(X_train, y_train, KLR, ax1, \"Training Accuracy = \")\nconstruct_plot(X_test, y_test, KLR, ax2, \"Test Accuracy = \")\n\n\n\n\n\n\nExperiment 2: Choosing Gamma\nIn this experiment I will demonstrate the importance of choosing a good gamma value. A gamma too low will underfit the data, but a gamma too high will overfit the data. As we can see values for gamma above 1 begin to lose accuracy on the test set.\n\nX_train, y_train = make_moons(80, shuffle = True, noise = 0.2)\nX_test, y_test = make_moons(80, shuffle = True, noise = 0.2)\ngammas = [.01, .1, 1, 10, 100, 1000, 10000]\nfig, axarr = plt.subplots(len(gammas), 2, figsize=(12, 6*len(gammas)))\n\nfor i, gamma in enumerate(gammas):\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"gamma={gamma} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"gamma={gamma} Test Accuracy = \")\n\n\n\n\n\n\nExperiment 3: Varied Noise\nIn this experiment I vary the noise paramater of the make_moons function, and I test what values of gamma are best with increased noise.\nIn the last experiment we tested with noise=.2 and saw that a gamma value of around 1 was sufficient.\n\nTesting with Noise=.05\nFrom this test we can gather that when there is very little noise (and very little difference between the train and test data), we just need a high enough gamma to follow the shape.\n\nX_train, y_train = make_moons(80, shuffle = True, noise = 0.05)\nX_test, y_test = make_moons(80, shuffle = True, noise = 0.05)\ngammas = [.01, .1, 1, 10]\nfig, axarr = plt.subplots(len(gammas), 2, figsize=(12, 6*len(gammas)))\n\nfor i, gamma in enumerate(gammas):\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"gamma={gamma} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"gamma={gamma} Test Accuracy = \")\n\n\n\n\n\n\nTesting with Noise=.04\nIn this test we see that a gamma of .1 does a the best at matching the shape we want. Higher than .1 is overfit and below .1 is underfit.\n\nX_train, y_train = make_moons(80, shuffle = True, noise = 0.4)\nX_test, y_test = make_moons(80, shuffle = True, noise = 0.4)\ngammas = [.01, .1, 1, 10]\nfig, axarr = plt.subplots(len(gammas), 2, figsize=(12, 6*len(gammas)))\n\nfor i, gamma in enumerate(gammas):\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"gamma={gamma} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"gamma={gamma} Test Accuracy = \")\n\n\n\n\n\n\nConclusions from Experiment 3\nOverall, I donâ€™t have enough evidence to suggest that the best value for gamma depends on the noise level. This generally matches what I have seen in the documentation that assigns gamma based off of the size of the training set.\n\n\n\nExperiment 4: Other Shapes\n\nTesting with gamma=.1\n\nnoises = [0, .1, .2]\nfig, axarr = plt.subplots(len(noises), 2, figsize=(12, 6*len(noises)))\ngamma = .1\nfor i, noise in enumerate(noises):\n    X_train, y_train = make_circles(50, shuffle = True, noise = noise)\n    X_test, y_test = make_circles(50, shuffle = True, noise = noise)\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"noise={noise} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"noise={noise} Test Accuracy = \")\n\n\n\n\n\n\nTesting with gamma=1\n\nnoises = [0, .1, .2]\nfig, axarr = plt.subplots(len(noises), 2, figsize=(12, 6*len(noises)))\ngamma = 1\nfor i, noise in enumerate(noises):\n    X_train, y_train = make_circles(50, shuffle = True, noise = noise)\n    X_test, y_test = make_circles(50, shuffle = True, noise = noise)\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"noise={noise} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"noise={noise} Test Accuracy = \")\n\n\n\n\n\n\nTesting with gamma=10\nA gamma value of 10 is too high with\n\nnoises = [0, .1, .2]\nfig, axarr = plt.subplots(len(noises), 2, figsize=(12, 6*len(noises)))\ngamma = 10\nfor i, noise in enumerate(noises):\n    X_train, y_train = make_circles(50, shuffle = True, noise = noise)\n    X_test, y_test = make_circles(50, shuffle = True, noise = noise)\n    KLR = KernelLogisticRegression(rbf_kernel, gamma=gamma)\n    KLR.fit(X_train, y_train)\n    construct_plot(X_train, y_train, KLR, axarr[i][0], f\"noise={noise} Training Accuracy = \")\n    construct_plot(X_test, y_test, KLR, axarr[i][1], f\"noise={noise} Test Accuracy = \")\n\n\n\n\n\n\nConclusions from Experiment 4\nWhen there is no noise pretty much any reasonable gamma will do, but once there starts to be signficant noise, the lower gamma value does better on this set."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Source code is here.\nIn this blog post I have implemented logistic regression.\n\nfrom logreg import LogisticRegression\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ndef draw_line(w, x_min, x_max, color=\"black\", ax=None, alpha=1):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  if ax is None:\n    plt.plot(x, y, color = color, alpha=alpha)\n  else:\n    ax.plot(x, y, color = color, alpha=alpha)"
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiments",
    "href": "posts/logistic-regression/index.html#experiments",
    "title": "Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\n\nExperiment 1: Alpha Too Large\nIn this experiment I train two models on the same data with the same initial weights and the same number of epochs in order to demonstrate what can happen when alpha is too high.\nIn the charts and graphs below, the blue corresponds to the model trained with alpha=10 and the red corresponds with alpha=20.\nIn the scatter plots, I plot all of the best fit lines, increasing the opacity of the lines to 100% as we approach the final epoch. In the alpha=10 plot, we see that the line quickly finds roughly where it needs to be and then makes very small changes it approaches the optimal line. In the alpha=20 plot, the best fit lines are all over the place. We see why if we look at the loss graph. Every time the alpha=20 model gets close to a very low loss, it passes the minimum and the loss jumps back up again.\n\nnp.random.seed(38532555)\np_features = 3\nX, y = make_blobs(n_samples = 80, n_features = p_features - 1, centers = [(-1.5, -1.5), (-2, 1.5)])\n\nsteps=300\n# train normal logistic regression\nreg_alpha_LR = LogisticRegression()\nreg_alpha_LR.fit(X, y, initial_w=np.array([1, 1, -1]), alpha = 10, max_epochs = steps, track_w=True)\n\n# train high alpha logistic regression\nhigh_alpha_LR = LogisticRegression()\nhigh_alpha_LR.fit(X, y, initial_w=np.array([1, 1, -1]), alpha = 20, max_epochs = steps, track_w=True)\n\nfig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 12))\n\n# figs 1 and 2\nfig1 = ax1.scatter(X[:,0], X[:,1], c = y)\nfig4 = ax4.scatter(X[:,0], X[:,1], c = y)\nfor i, (wi1, wi2) in enumerate(zip(reg_alpha_LR.w_history, high_alpha_LR.w_history)):\n    fig1 = draw_line(wi1, -3.5, 0, \"blue\", ax=ax1, alpha=((i+1+5)/(steps+5)))\n    fig4 = draw_line(wi2, -3.5, 0, \"red\", ax=ax4, alpha=((i+1+5)/(steps+5)))\n    \nax1.set(xlabel='Feauture 1', ylabel='Feauture 2', title=\"alpha=10\")\nax4.set(xlabel='Feauture 1', ylabel='Feauture 2', title=\"alpha=20\")\n\n# fig 2\nnum_steps = len(reg_alpha_LR.loss_history)\nax2.plot(np.arange(num_steps) + 1, reg_alpha_LR.loss_history, color=\"blue\")\nax2.set(xlabel='Epoch', ylabel='Loss')\n\n#fig 5\nnum_steps = len(high_alpha_LR.loss_history)\nax5.plot(np.arange(num_steps) + 1, high_alpha_LR.loss_history, color=\"red\")\nax5.set(xlabel='Epoch', ylabel='Loss')\n\n# fig 3\nnum_steps = len(reg_alpha_LR.score_history)\nax3.plot(np.arange(num_steps) + 1, reg_alpha_LR.score_history, color=\"blue\")\nax3.set(xlabel='Epoch', ylabel='Score')\n\n# fig 6\nnum_steps = len(high_alpha_LR.score_history)\nax6.plot(np.arange(num_steps) + 1, high_alpha_LR.score_history, color=\"red\")\nax6.set(xlabel='Epoch', ylabel='Score')\nNone\n\n\n\n\n\n\nExperiment 2 and 3: Batch Size and Momentum\nIn this experiment I will demonstrate how small batch gradient descent can boost the speed of an algorithm, and then how momentum can boost speed on top of that.\nAll models are trained with the same data, starting weights, and number of epochs.\nThe data has five features, so I will not be showing any best fit lines. Instead, I chart the loss and score histories. These cleary show how the non sped up gradient descent has not even come close to reaching a minimum after 250 epochs. The stochasitc descent took over 200 epochs, but the stochastic with momentum took only about 50.\n\nnp.random.seed(385325)\np_features = 6\nX, y = make_blobs(n_samples = 400, n_features = p_features - 1, centers = [(-1, 1, 0, -1, -1), (1, -1, 2, 1, 1)])\n\nsteps=250\n# train normal logistic regression\nreg_LR = LogisticRegression()\nreg_LR.fit(X, y, initial_w=[-1, 4, -4, 1, 3, -1], alpha = .01, max_epochs = steps)\n\n# train small batch logistic regression\nstochastic_LR = LogisticRegression()\nstochastic_LR.fit_stochastic(X, y, initial_w=[-1, 4, -4, 1, 3, -1], batch_size=25, alpha = .01, max_epochs = steps)\n\n# train small batch logistic regression with momentum\nstoch_momentum_LR = LogisticRegression()\nstoch_momentum_LR.fit_stochastic(X, y, initial_w=[-1, 4, -4, 1, 3, -1], momentum=.8, batch_size=25, alpha = .01, max_epochs = steps)\n\nfig, (ax2, ax3) = plt.subplots(1, 2, figsize=(18, 6))\n\n# fig 2\nnum_steps = len(reg_LR.loss_history)\nax2.plot(np.arange(num_steps) + 1, reg_LR.loss_history, label=\"regular gradient\")\nax2.plot(np.arange(num_steps) + 1, stochastic_LR.loss_history, label=\"stochastic\")\nax2.plot(np.arange(num_steps) + 1, stoch_momentum_LR.loss_history, label=\"stochastic w/momentum\")\nax2.set(xlabel='Epoch', ylabel='Loss')\nlegend = ax2.legend() \n\n# fig 3\nnum_steps = len(reg_LR.score_history)\nax3.plot(np.arange(num_steps) + 1, reg_LR.score_history, label=\"regular gradient\")\nax3.plot(np.arange(num_steps) + 1, stochastic_LR.score_history, label=\"stochastic\")\nax3.plot(np.arange(num_steps) + 1, stoch_momentum_LR.score_history, label=\"stochastic w/momentum\")\nax3.set(xlabel='Epoch', ylabel='Score')\nlegend = ax3.legend()"
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Source code is here.\nIn this blog post I have implemented least-squares linear regression, which is linear regression using a least-squares cost function. Minimizing the least squares cost function actually has an analytical solution, which I have implemented in addition to gradient descent.\n\nfrom linreg import LinearRegression # your source code\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef draw_line(w, x_min, x_max, *, color=\"black\", ax=None, alpha=1, **kwargs):\n  x = np.linspace(x_min, x_max, 101)\n  if len(w) == 3:\n    y = -(w[0]*x + w[2])/w[1]\n  elif len(w) == 2:\n    y = w[0]*x + w[1]\n  if ax is None:\n    plt.plot(x, y, color = color, alpha=alpha, **kwargs)\n  else:\n    ax.plot(x, y, color = color, alpha=alpha, **kwargs)\n\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# train\nLR_analytical = LinearRegression()\nLR_analytical.fit_analytical(X_train, y_train) \nLR_gradient = LinearRegression()\nLR_gradient.fit_gradient(X_train, y_train, w=[.5, .5], max_steps=100, alpha=.005)\n\n# plot best fit lines\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\naxarr[0].scatter(X_train, y_train, color=\"gray\", alpha=.5, label=\"Train\", s=15)\naxarr[0].scatter(X_val, y_val, color=\"black\", alpha=1, label=\"Validation\", s=20)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\n\ndraw_line(LR_analytical.w, 0, 1, color=\"blue\",  ax=axarr[0], label=\"Analytical\", lw=\"2\")\ndraw_line(LR_gradient.w, 0, 1, color=\"red\", ax=axarr[0], label=\"Gradient\", linestyle=\"dotted\", lw=\"4\")\naxarr[0].legend()\n\n# plot score\naxarr[1].plot(LR_gradient.score_history)\nlabels = axarr[1].set(xlabel = \"Iteration\", ylabel = \"Score\", title = \"Score Through Training\")\naxarr[1].set_ylim([0, 1])\n\nplt.tight_layout()\n\nprint(\"\\nAnalytical method:\")\nprint(f\"Training score = {LR_gradient.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR_gradient.score(X_val, y_val).round(4)}\")\n\nprint(\"\\nGradient method:\")\nprint(f\"Training score = {LR_gradient.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR_gradient.score(X_val, y_val).round(4)}\")\n\n\nAnalytical method:\nTraining score = 0.4919\nValidation score = 0.4647\n\nGradient method:\nTraining score = 0.4919\nValidation score = 0.4647"
  },
  {
    "objectID": "posts/linear-regression/index.html#experiments",
    "href": "posts/linear-regression/index.html#experiments",
    "title": "Linear Regression",
    "section": "Experiments",
    "text": "Experiments\n\nExperiments 1 and 2: Many Features and LASSO Regularization\nIn this experiment we will increase the number of features up to n - 1 in order to study what happens to the training and validation scores.\nWe will also add use a LR model that adds a regularizing term to its loss function to fight against overfitting when the number of features is very high.\n\nfrom sklearn.linear_model import Lasso\n\nn_train = 100\nn_val = 100\nnoise = 0.2\nscores = []\nscores_lasso = []\nfor i in range(1, n_train):\n    p_features = i\n\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n    LR = LinearRegression()\n    LR.fit_analytical(X_train, y_train)     \n    \n    LR_lasso = Lasso(alpha = 0.001)\n    LR_lasso.fit(X_train, y_train)\n    \n    scores.append({\"train\": LR.score(X_train, y_train), \"validation\": LR.score(X_val, y_val)})\n    scores_lasso.append({\"train\": LR_lasso.score(X_train, y_train), \"validation\": LR_lasso.score(X_val, y_val)})\n\n# plot score\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n\nscores_df = pd.DataFrame(scores)\nscores_df.index = np.arange(1, len(scores_df) + 1)\nscores_df.plot(ax=ax0, xlabel=\"Number of features\", ylabel=\"Score\")\nax0.set_ylim([0, 1.05])\n\nscores_lasso_df = pd.DataFrame(scores_lasso)\nscores_lasso_df.index = np.arange(1, len(scores_lasso_df) + 1)\nscores_lasso_df.plot(ax=ax1, xlabel=\"Number of features\", ylabel=\"Score\")\n\nprint(f\"Scores with {n_train} training samples and {n_train-1} features:\")\nprint(f\"Training score = {round(scores[-1]['train'], 4)}\")\nprint(f\"Validation score = {round(scores[-1]['validation'], 4)}\")\n\nprint(f\"\\nScores while using modified loss function with regularization term:\")\nprint(f\"Training score = {round(scores_lasso[-1]['train'], 4)}\")\nprint(f\"Validation score = {round(scores_lasso[-1]['validation'], 4)}\")\n\nScores with 100 training samples and 99 features:\nTraining score = 1.0\nValidation score = 0.5331\n\nScores while using modified loss function with regularization term:\nTraining score = 0.9982\nValidation score = 0.8288\n\n\n\n\n\nAs we can clearly see, our implementation becomes severely overfit as the number of features approaches the number of training examples. The training score approaches near perfection, whereas the validation score gets worse.\nThe scikit-learn implementation with the regularization term also exhibits some pretty serious overfitting, but not to the same degree as our implementation. When the number of features is nearly equal to the number of training examples, the regularization term is able to keep the validation score from tanking.\n\n\nExperiment 3: Bike Share Data\nIn this experiment we will train a model to predict the number of riders of a bikesharing program in DC.\n\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\",\n        \"dteday\"]\n\nbikeshare = data[cols]\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\n\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\", \"dteday\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\", \"dteday\"], axis = 1)\ny_test = test[\"casual\"]\n\nLR = LinearRegression()\nLR.fit_analytical(X_train, y_train)\n\npreds_train = LR.predict(X_train)\npreds_test = LR.predict(X_test)\n\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(data['dteday']), data['casual'], label=\"Actual\")\nax.plot(pd.to_datetime(train['dteday']), preds_train, label=\"Training Set Predictions\")\nax.plot(pd.to_datetime(test['dteday']), preds_test, label=\"Test Set Predictions\")\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nax.legend()\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_test, y_test).round(4)}\")\n\nTraining score = 0.7318\nValidation score = 0.6968\n\n\n\n\n\n\nX_train\n\n\n\n\n\n  \n    \n      \n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      579\n      1\n      1\n      1\n      0.752500\n      0.659583\n      0.129354\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      580\n      2\n      1\n      1\n      0.765833\n      0.642500\n      0.215792\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      581\n      1\n      0\n      1\n      0.793333\n      0.613333\n      0.257458\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      582\n      1\n      0\n      1\n      0.769167\n      0.652500\n      0.290421\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      583\n      2\n      1\n      1\n      0.752500\n      0.654167\n      0.129354\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n584 rows Ã— 18 columns\n\n\n\n\nprint(\"Weights for each feature\")\nfor i, (key, weight) in enumerate(zip(X_train.keys(), LR.w)):\n    print(f\" {i+1}. {key}: {weight}\")\n\nWeights for each feature\n 1. weathersit: -108.37113626599087\n 2. workingday: -791.6905491329867\n 3. yr: 280.58692732526924\n 4. temp: 1498.7151127165637\n 5. hum: -490.10033978027354\n 6. windspeed: -1242.8003807505336\n 7. holiday: -235.8793491765524\n 8. mnth_2: -3.3543971201747707\n 9. mnth_3: 369.2719555186909\n 10. mnth_4: 518.4087534451241\n 11. mnth_5: 537.3018861590382\n 12. mnth_6: 360.8079981484559\n 13. mnth_7: 228.88148124909466\n 14. mnth_8: 241.3164120150012\n 15. mnth_9: 371.50385386758205\n 16. mnth_10: 437.6008478681119\n 17. mnth_11: 252.43300404938137\n 18. mnth_12: 90.8214604976828\n\n\n\nExamining the weights:\nWe can see that our model found temperature to be the single largest factor in determining number of riders. It gave negative weights to weekdays and surprisingly holidays as well. The warmer months have higher weights than the winter months, but early spring and fall have higher weights than summer does. I believe that the temperature weight compensates for the lower weights of the summer months to make it so the model still predicts the highest ridership numbers during the summer. This makes sense, since every day during the summer in DC is hotâ€¦ The year also was given a positive weight, reflecting the higher ridership numbers in 2012 compared to 2011."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Experiment 1: Linearly Separable 2D Data\nThis experiment looks at linearly seperable data with 2 features. The perceptron is able to quickly find a line that achieves 100% accuracy.\n\nnp.random.seed(12345)\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.8, -1.9), (1.6, 1.7)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 2: NOT Linearly Separable 2D Data\nThis experiment looks at data with 2 features that is not linearly seperable. The perceptron is not able to find a set of weights (a line) that can accuractely predict all of the data points because the data because this data is not linearly seperable.\nAlso, one of the qualities of the perceptron is that it does not converge, and we can see in the graph below containing the accuracies over the iterations that the accuracy continues to jump up and down until it reaches the max number of steps.\n\nnp.random.seed(54321)\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 3: Linearly Separable (?) 5D Data\nThis data containing 5 features turns out to be linearly seperable. We can tell it is linearly seperable without visualizing it because the perceptron is able to find line weights that make predictions with an accuracy of 100%.\n\nnp.random.seed(12321)\nn = 100\np_features = 6\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.75, -1.75), (1.75, 1.75)])\n\np = Perceptron()\np.fit(X, y, steps=1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThis data is linearly seperable. The perceptron was able to achieve 100% accuracy!\n\n\nRuntime Complexity of the Update Step\nThe update contains multiple individual steps occuring in sequence. We will look at them all individually\nFirst lets look at making the prediction: np.sign(np.dot(self.w, X_[i])). This operation does a 1xp dot px1 operation. This specific step is O(p).\nNext, lets look at the forming of the gate: (y_[i] * y_pred) < 0. This is clearly O(1).\nNow for the update: self.w + false_check * (y_[i] * X_[i]). The major component here is the scalar * vector operation. Since the vector is of length p, this is O(p) as well.\nWe could end there and be done, but we check the overall accuracy as part of the update step: (np.dot(self.w, X_.T) > 0). This is a dot product operation of 1xp vector and a pxn matrix. Therefore, this step has a runtime of O(pn).\nSince all of the steps are done in sequence, the runtime for the total update step is O(pn)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that youâ€™ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog post #5: auditing allocative bias.\n\n\n\n\n\n\nApr 11, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #4: linear regression.\n\n\n\n\n\n\nApr 8, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #2: logistic regression.\n\n\n\n\n\n\nMar 29, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #3: kernel logistic regression.\n\n\n\n\n\n\nMar 8, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post #1: perceptrons.\n\n\n\n\n\n\nFeb 20, 2023\n\n\nJack Greenburg\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques youâ€™ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog!"
  },
  {
    "objectID": "posts/auditing-allocative-bias/index.html",
    "href": "posts/auditing-allocative-bias/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "Source code is here.\nIn this blog post I create a model that predicts employment status from a few features such as age, sex, and disability status, and I then analyze it for racial bias. Notably, race was not one of the features used, but its effects as a feature can be mimicked from combinations of other features.\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\n\n# I chose to stay using Alabama, since they didn't seem to have DC. Roll Tide\nSTATE = \"AL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nI picked these features:\n\nSCHL - level of schooling\nMAR - marital status\nAGEP - age\nMIL - military experience\nDIS - disability status\nSEX\n\nAlso note, for RAC1P:\n\nWhite alone\nBlack or African American alone\nAmerican Indian alone\nAlaska Native alone\nAmerican Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races\nAsian alone\nNative Hawaiian and Other Pacific Islander alone\nSome Other Race alone\nTwo or More Races\n\nBut 4, 5, and 6 did not have enough examples, so I folded them into the Some Other Race category. The new indices are:\n\nWhite alone\nBlack or African American alone\nAmerican Indian alone\nAsian alone\nSome Other Race alone\nTwo or More Races\n\n\nfeature_categories = [\"SCHL\", \"MAR\", \"AGEP\", \"DIS\", \"SEX\", \"NATIVITY\"]\n\nEmploymentProblem = BasicProblem(\n    features=feature_categories,\n    target='ESR',\n    target_transform=lambda x: x == 1,  # 1 means employed\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n# fold some races into \"other\" category\ngroup[(group==4) | (group==5) | (group==7)] = 8\ngroup[group==6] = 4\ngroup[group==8] = 5\ngroup[group==9] = 6\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\ndf = pd.DataFrame(X_train, columns = feature_categories)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\n\n\nraces = [\n    \"White\",\n    \"Black\",\n    \"American Indian\",\n    \"Asian\",\n    \"Other Race\",\n    \"Two or More Races\"\n]\n\nprint(f\"1. There are {df.shape[0]} individuals in the dataset.\")\nprint(f\"2. Of those individuals, {df.label.sum()} are employed\")\nprint(f\"3, 4. Number and ratio employed by group:\")\n\nemployee_races = df[df[\"label\"] == True][\"group\"]\nemployed_per_group = employee_races.value_counts(sort=False)\nratio_employed_per_group = employed_per_group/df[\"group\"].value_counts(sort=False)\n\nprint(\"     Race\".ljust(28) + \"Employed\".rjust(10) + \"Ratio Employed\".rjust(20))\nfor index, race in enumerate(races):\n    line = f\"  {index+1}. {race}:\".ljust(28)\n    line += f\"{employed_per_group[index+1]} \".rjust(10)\n    line += f\"{round(ratio_employed_per_group[index+1] * 100, 2)}% \".rjust(20)\n    print(line)\n\n# plot disability race intersection\ngroup_dict = {i+1: status for i, status in enumerate(races)}\nworking_by_race = df.query(\"DIS==1\")[[\"label\", \"group\"]].value_counts(sort=False)\nfig, axarr = plt.subplots(1, 6, figsize=(15, 5))\nfor i, (index, race) in enumerate(group_dict.items()):\n    axarr[i].pie((working_by_race[False][index], working_by_race[True][index]), autopct='%1.1f%%')\n    axarr[i].set_title(race)\nplt.legend(labels=[\"Not employed\", \"Employed\"], loc=\"lower right\")\nprint(f\"5. A plot showing percent of those who have a disability who are employed, by race:\")\n\n1. There are 38221 individuals in the dataset.\n2. Of those individuals, 15638 are employed\n3, 4. Number and ratio employed by group:\n     Race                     Employed      Ratio Employed\n  1. White:                     12020              42.26% \n  2. Black:                      2976              36.88% \n  3. American Indian:              68              43.59% \n  4. Asian:                       226               50.0% \n  5. Other Race:                  157              35.76% \n  6. Two or More Races:           191              28.81% \n5. A plot showing percent of those who have a disability who are employed, by race:\n\n\n\n\n\n\n\n\n\nI performed a grid search with max_depth=[5, 10] and max_features=[10, 20, 50] and it found max_depth=5 and max_features=10 to be optimal.\n\n# parameters = {\"gradientboostingclassifier__max_depth\": (5, 10),\n#               \"gradientboostingclassifier__max_features\": (10, 20, 50)}\n# pipeline = make_pipeline(StandardScaler(), GradientBoostingClassifier())\n# clf = GridSearchCV(pipeline, \n#                    parameters, \n#                    scoring=\"accuracy\")\n# clf.fit(X_train, y_train)\n\npipeline = make_pipeline(StandardScaler(), GradientBoostingClassifier(max_depth=5, max_features=10))\nclf = pipeline.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\npipeline.get_params\n\n<bound method Pipeline.get_params of Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('gradientboostingclassifier',\n                 GradientBoostingClassifier(max_depth=5, max_features=10))])>\n\n\n\n\n\n\n\n\ndef compute_measures(*, y_test, y_pred, print_bool=False):\n    (TN, FP), (FN, TP) = confusion_matrix(y_test, y_pred)\n    data = {\n        \"Accuracy\": (y_pred == y_test).mean(),\n        \"PPV\": TP / (TP+FP),\n        \"FPR\": FP / (FP+TN),\n        \"FNR\": FN / (TP+FN)       \n    }\n\n    data[\"Predicted Employed\"] = (TP + FP) / (TP + FP + TN + FN)\n\n    if print_bool: \n        for key, value in data.items(): print(f\"{key}: {round(value * 100, 2)}%\")\n    return data\n\nprint(\"All data:\")\ncompute_measures(y_test=y_test, y_pred=y_pred, print_bool=True)\n\naudit_dict = {}\nfor index, race in group_dict.items():\n    # print(f\"\\n{race} subgroup:\")\n    audit_dict[race] = compute_measures(y_test=y_test[group_test==index], y_pred=y_pred[group_test==index])\npd.DataFrame(audit_dict).T\n\nAll data:\nAccuracy: 81.07%\nPPV: 76.35%\nFPR: 17.0%\nFNR: 21.69%\nPredicted Employed: 42.26%\n\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      PPV\n      FPR\n      FNR\n      Predicted Employed\n    \n  \n  \n    \n      White\n      0.809845\n      0.776081\n      0.169746\n      0.217306\n      0.432771\n    \n    \n      Black\n      0.814670\n      0.722981\n      0.170620\n      0.211382\n      0.393643\n    \n    \n      American Indian\n      0.769231\n      0.700000\n      0.187500\n      0.300000\n      0.384615\n    \n    \n      Asian\n      0.711538\n      0.690909\n      0.320755\n      0.254902\n      0.528846\n    \n    \n      Other Race\n      0.868687\n      0.864865\n      0.084746\n      0.200000\n      0.373737\n    \n    \n      Two or More Races\n      0.831395\n      0.666667\n      0.144000\n      0.234043\n      0.313953\n    \n  \n\n\n\n\n\nprint(\"Ground truth percentages employed for reference:\")\ntrue_employment_percentages = df.query('label==True')['group'].value_counts(sort=False) / df['group'].value_counts(sort=True)\nfor (index, race), percentage in zip(group_dict.items(), true_employment_percentages):\n    print(f\"{index}. {race}: {round(percentage*100, 2)}%\")\n\nGround truth percentages employed for reference:\n1. White: 42.26%\n2. Black: 36.88%\n3. American Indian: 43.59%\n4. Asian: 50.0%\n5. Other Race: 35.76%\n6. Two or More Races: 28.81%\n\n\n\n\n\nIs your model approximately calibrated?\nMy model should be calibrated. The score threshold for a positive prediction should be the same across all samples in the dataset.\nDoes your model satisfy approximate error rate balance?\nMy model does not fully satisfy error rate balance. The two largest groups (white and black) have similar FPR (~17%) and FNR (~22%), but many of the classes with lower prevalence have quite different rates.\nDoes your model satisfy statistical parity?\nMy model does not satisfy statistical parity. Predicted employment percentages vary over a range of about 20 percentage points.\n\n\n\n\n\n\nbias_df = pd.DataFrame(X_test, columns = feature_categories)\nbias_df[\"group\"] = group_test\nbias_df[\"label\"] = y_test\n\nintersectional_audit_dict = {}\nfor index, race in group_dict.items():\n    for subgroup in [1, 2]:\n        # print(f\"\\n{race} subgroup:\")\n        include_bool_arr = (group_test==index) | (bias_df[\"DIS\"] == subgroup)\n        intersectional_audit_dict[f\"With{['', 'out'][subgroup-1]} Disability, {race}\"] = compute_measures(\n            y_test=y_test[include_bool_arr], \n            y_pred=y_pred[include_bool_arr]\n        )\npd.DataFrame(intersectional_audit_dict).T\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      PPV\n      FPR\n      FNR\n      Predicted Employed\n    \n  \n  \n    \n      With Disability, White\n      0.812352\n      0.774298\n      0.156832\n      0.231657\n      0.408683\n    \n    \n      Without Disability, White\n      0.808649\n      0.764808\n      0.181000\n      0.205327\n      0.442121\n    \n    \n      With Disability, Black\n      0.825578\n      0.716609\n      0.099673\n      0.363261\n      0.251975\n    \n    \n      Without Disability, Black\n      0.805474\n      0.765445\n      0.208575\n      0.177550\n      0.486559\n    \n    \n      With Disability, American Indian\n      0.844875\n      0.623377\n      0.019256\n      0.839465\n      0.042659\n    \n    \n      Without Disability, American Indian\n      0.802623\n      0.766054\n      0.224994\n      0.166118\n      0.510608\n    \n    \n      With Disability, Asian\n      0.838470\n      0.636364\n      0.028479\n      0.771513\n      0.064293\n    \n    \n      Without Disability, Asian\n      0.802648\n      0.766365\n      0.224861\n      0.166256\n      0.510671\n    \n    \n      With Disability, Other Race\n      0.847304\n      0.699029\n      0.020052\n      0.779817\n      0.054992\n    \n    \n      Without Disability, Other Race\n      0.802621\n      0.766113\n      0.224831\n      0.166301\n      0.510409\n    \n    \n      With Disability, Two or More Races\n      0.844525\n      0.638655\n      0.026841\n      0.772455\n      0.061467\n    \n    \n      Without Disability, Two or More Races\n      0.802746\n      0.765920\n      0.224529\n      0.166301\n      0.509882\n    \n  \n\n\n\n\nThe false negative rates here are really interesting. It seems that the model does not significantly alter its prediction based off disability for white people, but for every other racial category the changes are quite large. These numbers almost make sense when you look at the above pie plots: Asianâ€™s and American Indianâ€™s with disabilities are twice as likely to be working. This would partially explain why their false negative rates are high, but it wouldnâ€™t explain the other races with very high FNRs.\n\n\n\nfor feat, importance in zip(feature_categories, clf[1].feature_importances_):\n    print(f\"{feat}:, {round(importance*100, 2)}%\")\n\nSCHL:, 36.14%\nMAR:, 2.65%\nAGEP:, 46.12%\nDIS:, 11.35%\nSEX:, 3.1%\nNATIVITY:, 0.64%\n\n\n\n\n\n\n\nA model trained to predict unemployment could be used for a variety of purposes. Here are two examples where it could be used:\n\nIt could be used to direct job ads to people that are unemployed, hoping to find people that are looking for work.\nIt could be used to direct job ads to only people that are employed, viewing the unemployed as undesirable.\n\nIn this sense, depending on its application, it could both help and hurt unemployed people. If we take American Indians as an example, since my model only had a very low FPR for them, they would be less likely to be excluded from job ads targeted to unemployed people, and more likely to be excluded from ads targeted to employed people.\nThe actual real world applications for my model specifically are slim. For starters, it is a very mediocre classifier, but, also, a model trained to predict employment status from the identifiers that I used could only be used by somebody with access to a bizarre dataset. The features I used are generally not easier to come by than employment status is to gain directly. Furthermore, the features that are harder to acquire are the more important ones. Perhaps Instagram or Facebook could analyze your account to come up with age, sex, and relationship status, but level of schooling and disability seem much harder, and for little payoff. To put it plainly, anybody that really needs employment status usually has the ability to ask directly, and if they canâ€™t ask then theyâ€™re probably not going to have the data necessary to predict it.\nI think my model does display some problematic bias. The most concerning issue to me is the error rate balance. More specifically, I am concerned by how high the FPR is for American Indians and how high the FNR is for Asians. Most of the other values are relatively in line with each other, but these two stand out. If this model were to be implemented it, then American Indians and Asians would not be properly represented.\nBeyond concerns of bias, I think my model is incredibly innocuous. A model that predicted employment status through gait or clothing would be much more dangerous since it could be employed so much wider. The issue of acquiring the data necessary makes my model highly impractical to actually use by someone with malicious intentions."
  }
]